---
title: LaplacianMatrix
date: 2023-05-26 23:06:32
tags: 
    - 拉普拉斯矩阵
    - 图正则化

categories: 
    - 基础知识
    - 图神经网络

mathjax: true
---

# 1. 图拉普拉斯矩阵的定义

给定图 $\mathcal{G} = (\mathcal{V}, \mathcal{E})$，其拉普拉斯矩阵的定义如下：
$$ L = D - A = \begin{cases}
deg(i), & i==j \\
-1, & e_{ij} \in  \mathcal{E} \\
0, & otherwise
\end{cases}$$
其中 $D$ 表示度矩阵，$A$ 为邻接矩阵。我们可以看出，拉普拉斯矩阵主对角线第 $i$ 个矩阵表示第 $i$ 个节点的度， 即 
$$D_{ii} = \sum_{j} A_{ij}$$
可以看出拉普拉斯矩阵是一个实对称阵，且行元素之和为0。

# 2. 拉普拉斯算子
拉普拉斯矩阵的定义来源于拉普拉斯算子，后者是 $n$ 维欧氏空间的二阶微分算子(用于计算散度)
$$\Delta f = \nabla^2 f = \nabla \cdot \nabla f = \sum_{i=1}^n \frac{\partial ^2 f}{\partial x_i^2}$$
如果把图拉普拉斯矩阵看作是线性变换的话，它的作用与数学分析中的拉普拉斯算子是一样的。下面使用泰勒级数来推导。

首先假设离散空间最少单位步长单位 $h$，即
$$x_{i+1} - x_i = h$$
$$x_i - x_{i-1} = h$$
然后使用泰勒级数将函数 $f(x_{i+1})$ 和 $f(x_{i-1})$ 的函数值再 $x_i$ 处展开，可以得到：
$$f(x_{i+1}) = f(x_i) + f^{'}(x_i)h + \frac{f^{''}(x_i)}{2!}h^2 + O(h^2) \tag{1}$$
$$f(x_{i-1}) = f(x_i) - f^{'}(x_i)h + \frac{f^{''}(x_i)}{2!}h^2 + O(h^2) \tag{2}$$
如果直接求解 $f^{'}(x_i)$，其截断误差都是 $O(h)$，为了进一步减小误差，可以使上下两式相减，可以得到
$$f^{'}(x_i) = \frac{f(x_{i+1})-f(x_{i-1})}{2} - O(h^2) \tag{3}$$
可以看到，（3）式的误差变成了 $O(h^2)$。利用同样的方法推导 $f^{''}(x_i)$，让（1）和（2）式相加，可以得到
$$f^{''}(x_i) = \frac{f(x_{i+1})+f(x_{i-1}) - 2f(x_i)}{h^2} - O(h^3) \tag{4}$$
因此，可以将其表示为
$$\Delta f = f(x_{i+1})+f(x_{i-1}) - 2f(x_i) $$
如果将其离散到二维空间，就变成了边缘检测算子，描述中心像素与局部上下左右四个邻居之间的差异
$$\Delta f(x,y) = \left[ f(x+1, y) + f(x-1, y) + f(x, y+1) + f(x, y-1) \right]-4 f(x,y)$$

如果在图信号中，拉普拉斯算子被用来描述中心节点和邻居节点之间的差异，

$$Lx= \left[ \begin{array}{c} 
\sum_{j\in \mathcal{N}(1)} (x_1 - x_j) \\
\sum_{j\in \mathcal{N}(2)} (x_2 - x_j) \\
\vdots \\
\sum_{j\in \mathcal{N}(n)} (x_n - x_j) \\
\end{array}\right] $$
实际上拉普拉斯矩阵可以看做一个差分算子，第 $i$ 只与第 $i$ 个图节点及其一阶邻居节点有关，因此它反映了图信号局部平滑度的算子。
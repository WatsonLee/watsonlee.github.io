<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>约束优化</title>
    <link href="/2022/11/18/KKT/"/>
    <url>/2022/11/18/KKT/</url>
    
    <content type="html"><![CDATA[<p>Karush-Kuhn-Tucker（KKT）条件是非线性规划最佳解的必要条件。KKT条件将Lagrange乘数法所涉及到的等式约束优化问题推广至不等式。在实际应用上，KKT条件一般不存在代数解。许多优化算法可提供数值计算选用。</p><h1 id="原始问题">1. 原始问题</h1><h2 id="等式约束优化问题">1.1 等式约束优化问题</h2><p><a id="sec1.1"></a></p><p>给定一个目标函数 $f:^n  $，我们希望找到 <span class="math inline">\(\mathbf{x}\in \mathbb{R}^n\)</span>，在满足约束条件 <span class="math inline">\(g(\mathbf{x})=0\)</span> 的前提下，使得 <span class="math inline">\(f(\mathbf{x})\)</span> 有最小值，这个约束优化问题记为：</p><p><span class="math display">\[\begin{split}&amp; \min \quad f(\mathbf{x}) \\&amp; \text{s.t.} \quad g(\mathbf{x})=0\end{split}\tag{1}\]</span></p><p>为了方便分析，假设 <span class="math inline">\(f\)</span> 与 <span class="math inline">\(g\)</span> 均为连续可导函数。Lagrange乘数法是等式约束优化问题的典型解法。定义Lagrange函数：</p><p><span class="math display">\[L(\mathbf{x}, \lambda) = f(\mathbf{x}) + \lambda g(\mathbf{x}) \tag{2}\]</span></p><p>其中 <span class="math inline">\(\lambda\)</span> 为Lagrange乘数。Lagrange乘数法将原本的约束优化问题转化为等价的无约束优化问题：</p><p><span class="math display">\[\min\limits_{\mathbf{x},\lambda} L(\mathbf{x}, \lambda) \tag{3}\]</span></p><p>计算 <span class="math inline">\(L\)</span> 对 <span class="math inline">\(\mathbf{x}\)</span> 和 <span class="math inline">\(\lambda\)</span> 的偏导数并设为零，可以得到最优解的必要条件：</p><p><span class="math display">\[\begin{split}&amp; \nabla_\mathbf{x} L = \frac{\partial L}{\partial \mathbf{x}} = \nabla f + \lambda \nabla g = 0\\&amp; \nabla_\lambda L = \frac{\partial L}{\partial \lambda} = g(\mathbf{x}) = 0\end{split}\tag{4}\]</span></p><p>其中，公式（4）第一式为定常方程式（Stationary Equation），第二式为约束条件，求解上面 n+1个方程式可以得到 <span class="math inline">\(L(\mathbf{x}, \lambda)\)</span> 的Stationary point <span class="math inline">\(\mathbf{x}^{*}\)</span> 以及 <span class="math inline">\(\lambda\)</span> 的值（正负均有可能）。</p><h2 id="不等式约束优化问题">1.2 不等式约束优化问题</h2><p><a id="sec1.2"></a></p><p>将约束等式 <span class="math inline">\(g(\mathbf{x})=0\)</span> 推广为不等式 <span class="math inline">\(g(\mathbf{x}) \le 0\)</span>，考虑如下问题： <span class="math display">\[\begin{split}&amp; \min \quad f(\mathcal{x})\\&amp; \text{s.t.} \quad g(\mathcal{x}) \le 0\end{split}\tag{5}\]</span> 约束不等式 <span class="math inline">\(g(\mathbf{x}) \le 0\)</span>称为原始可行性（primal feasibility），据此我们定义可行域（feasible region）<span class="math inline">\(K = \{\mathbf{x}\in\mathbb{R}^n | g(\mathbf{x}) \le 0\}\)</span>，假设Stationary Point <span class="math inline">\(\mathbf{x}^*\)</span> 为满足约束条件的最佳解，分为以下两种情况讨论：</p><ul><li><span class="math inline">\(g(\mathbf{x}^*) &lt; 0\)</span>，表示最佳解位于可行域 <span class="math inline">\(K\)</span> 内部，称为内部解（interior solution），这时约束条件是不起作用的（inactive）</li><li><span class="math inline">\(g(\mathbf{x}^*) = 0\)</span>，表示最佳解落在可行域 <span class="math inline">\(K\)</span> 边界，称为边界解（boundary solution），此时约束条件发挥作用（active）</li></ul><p>这两种情况的最佳解具有不同的必要条件：</p><ul><li><p>内部解：在约束条件不发挥作用（inactive）的情况下， <span class="math inline">\(g(\mathbf{x})\)</span> 不起作用，约束问题退化为无约束优化问题，因此驻点 <span class="math inline">\(\mathbf{x}^*\)</span> 满足 <span class="math inline">\(\nabla_\mathbf{x} f = 0\)</span> 且 <span class="math inline">\(\lambda=0\)</span>。</p></li><li><p>边界解：在约束条件发挥作用（active）的情形下，约束不等式变成等式 <span class="math inline">\(g(\mathbf{x})=0\)</span>，这与Lagrange乘数法的情况相同。这里可以认为存在 <span class="math inline">\(\lambda\)</span> 使得 <span class="math inline">\(\nabla_\mathbf{x} f = -\lambda \nabla_\mathbf{x}g\)</span>。这里 <span class="math inline">\(\lambda\)</span> 的正负号是有其意义的。因此我们希望最小化 <span class="math inline">\(f\)</span>，梯度 <span class="math inline">\(\nabla_\mathbf{x} f\)</span> （函数 <span class="math inline">\(f\)</span> 在 <span class="math inline">\(\mathbf{x}^*\)</span> 点方向导数最大值，即最陡上升方向）应该指向可行域 <span class="math inline">\(K\)</span> 的内部（因为最优解最小值是在边界取得的），但 <span class="math inline">\(\nabla_\mathbf{x} g\)</span> 指向可行域 <span class="math inline">\(K\)</span> 外部（即 <span class="math inline">\(g(\mathbf{x})&gt;0\)</span> 的区域，因为约束是小于等于0，继续向外走才能持续使目标函数 <span class="math inline">\(f\)</span> 的值下降），因此 <span class="math inline">\(\lambda \ge 0\)</span>， 称为对偶可行性（dual feasibility）。</p></li></ul><h2 id="多个约束等式与约束不等式">1.3 多个约束等式与约束不等式</h2><p>根据<a href="#sec1.1">章节1.1</a>和<a href="#sec1.2">章节1.2</a>，我们可以推广至多个约束等式与约束不等式的情况，考虑标准约束优化（或者称非线性规划）：</p><p><span class="math display">\[\begin{split}\min\limits_{\mathbf{x}\in \mathbb{R}^n} \quad &amp;f(\mathbf{x})\\\text{s.t.} \quad &amp; c_i(\mathbf{x}) \le 0, \quad i=1,2,\ldots,k \\&amp; h_j(\mathbf{x}) = 0, \quad j = 1,2,\ldots, l\end{split}\tag{6}\]</span> 我们称上式为约束最优化问题为原始最优化问题或原始问题。</p><p>首先，引入Generalized Lagrange Function（广义拉格朗日函数）： <span class="math display">\[L(\mathbf{x}, \alpha, \beta) = f(\mathbf{x}) + \sum_{i=1}^k \alpha_i c_i(\mathbf{x}) + \sum_{j=1}^l \beta_j h_j(\mathbf{x}) \tag{7}\]</span> 这里 <span class="math inline">\(\alpha_i, \beta_j\)</span> 是Lagrange乘子，<span class="math inline">\(\alpha_i&gt;\ge 0\)</span>，考虑 <span class="math inline">\(\mathbf{x}\)</span> 的函数： <span class="math display">\[\theta_P(\mathbf{x}) = \max\limits_{\alpha, \beta; \alpha_i \ge 0} L(\mathbf{x}, \alpha, \beta) \tag{8}\]</span> 这里，下标 <span class="math inline">\(P\)</span> 表示原始问题。</p><p>假设给定某个 <span class="math inline">\(\mathbf{x}\)</span>，如果它违反原始问题的约束条件，即存在某个 <span class="math inline">\(i\)</span> 使得 <span class="math inline">\(c_i(\mathbf{x})&gt;0\)</span> 或者存在某个 <span class="math inline">\(j\)</span> 使得 <span class="math inline">\(h_j(\mathbf{x}) \neq 0\)</span>，那么就有</p><p><span class="math display">\[\theta_P(\mathbf{x}) = \max\limits_{\alpha, \beta; \alpha_i\ge 0} \left[f(\mathbf{x}) + \sum_{i=1}^k \alpha_i c_i (\mathbf{x}) + \sum_{j=1}^l \beta_j h_j (\mathbf{x}) \right] = + \infty \tag{9}\]</span> 因为如果某个 <span class="math inline">\(i\)</span> 使得约束 <span class="math inline">\(c_i(\mathbf{x})&gt;0\)</span>，则可令 <span class="math inline">\(\alpha_i \rightarrow + \infty\)</span>；若某个 <span class="math inline">\(j\)</span> 使得 <span class="math inline">\(h_j(\mathbf{x}) \neq 0\)</span>，则可令 <span class="math inline">\(\beta_j\)</span> 使 <span class="math inline">\(\beta_j h_j (\mathbf{x}) \rightarrow + \infty\)</span>， 而将其余各个 <span class="math inline">\(\alpha_i, \beta_j\)</span> 均取值为0.</p><p>相反地，如果 <span class="math inline">\(\mathbf{x}\)</span> 满足公式（6）中的约束条件式，则根据公式（7）和公式（8）可以得到： <span class="math display">\[\theta_P(\mathbf{x}) = \begin{cases}f(\mathbf{x}), \quad &amp; \mathbf{x} \text{满足原始问题约束}\\+ \infty, \quad &amp;\text{其他}\end{cases}\tag{10}\]</span> 所以，如果考虑极小化问题 <span class="math display">\[\min\limits_{\mathbf{x}}\theta_P(\mathbf{x}) = \min\limits_{\mathbf{x}} \max\limits_{\alpha, \beta;\alpha_i \ge 0} L(\mathbf{x}, \alpha, \beta) \tag{11}\]</span> 公式（11）是与公式（6）原始最优化问题是等价的，即它们有相同的解。问题 <span class="math inline">\(\min\limits_{\mathbf{x}} \max\limits_{\alpha, \beta;\alpha_i \ge 0} L(\mathbf{x}, \alpha, \beta)\)</span> 被称为广义拉格朗日函数的极小极大问题。这样一来，就把原始最优化问题表示为广义拉格朗日函数的极小极大问题。为了方便，可以定义原始问题的最优值 <span class="math display">\[p^* = \min\limits_{\mathbf{x}} \theta_P(\mathbf{x}) \tag{12}\]</span> 称为原始问题的值。</p><h1 id="对偶问题">2. 对偶问题</h1><p>定义 <span class="math display">\[\theta_D(\alpha, \beta) = \min\limits_{\mathbf{x}} L(\mathbf{x}, \alpha, \beta) \tag{13}\]</span> 再考虑极大化公式（13），即 <span class="math display">\[\max\limits_{\alpha, \beta;\alpha_i \ge 0} \theta_D(\alpha, \beta) = \max\limits_{\alpha,\beta;\alpha_i \ge 0} \min \limits_{\mathbf{x}} L(\mathbf{x}, \alpha, \beta) \tag{14}\]</span> 问题 <span class="math inline">\(\max\limits_{\alpha,\beta;\alpha_i \ge 0} \min \limits_{\mathbf{x}} L(\mathbf{x}, \alpha, \beta)\)</span> 被称为广义拉格朗日函数的极大极小问题。</p><p>可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题： <span class="math display">\[\begin{split}&amp;\max\limits_{\alpha, \beta} \theta_D(\alpha, \beta) = \max\limits_{\alpha, \beta}\min\limits_{\mathbf{x}} L(\mathbf{x}, \alpha, \beta) \\&amp;\text{s.t.} \quad \alpha_i \ge 0, \quad i=1,2,\ldots,k\end{split}\tag{15}\]</span> 上式被称为原始问题的对偶问题，定义对偶问题的最优值 <span class="math display">\[d^* = \max\limits_{\alpha, \beta; \alpha_i \ge 0} \theta_D (\alpha, \beta) \tag{16}\]</span> 为对偶问题的值。</p><h1 id="原问题与对偶问题之间的关系">3. 原问题与对偶问题之间的关系</h1><blockquote><p><strong>定理1:</strong> 如果原始问题和对偶问题都有最优值，则 <span class="math display">\[d^* = \max\limits_{\alpha, \beta;\alpha_i \ge 0} \min\limits_{\mathbf{x}} L(\mathbf{x}, \alpha, \beta) \le \min\limits_{\mathbf{x}}\max\limits_{\alpha, \beta;\alpha_i \ge 0} L(\mathbf{x}, \alpha, \beta) = p^* \tag{17} \]</span></p></blockquote><p><strong>证明：</strong> 根据公式（9）和公式（13）的定义，我们可以得到： <span class="math display">\[\theta_D(\alpha, \beta) =\min\limits_{\mathbf{x}} L(\mathbf{x}, \alpha, \beta) \le L(\mathbf{x}, \alpha, \beta) \le \max\limits_{\alpha, \beta; \alpha_i \ge 0} L(\mathbf{x}, \alpha, \beta) = \theta_P(\mathbf{x}) \tag{18}\]</span> 即 <span class="math display">\[\theta_D(\alpha, \beta) \le \theta_P(\mathbf{x}) \tag{19}\]</span> 由于原始问题和对偶问题都有最优值，所以： <span class="math display">\[\max\limits_{\alpha,\beta;\alpha_i \ge 0}\theta_D(\mathbf{x}) \le \min\limits_{\mathbf{x}} \theta_P(\mathbf{x}) \tag{20} \]</span> 因此，定理得证。</p><blockquote><p><em>推论1:</em> 设 <span class="math inline">\(x^*, \alpha^*, \beta^*\)</span> 分别是原始问题（公式（6））和对偶问题（公式（15））的可行解，并且 <span class="math inline">\(d^* = p^*\)</span>， 则 <span class="math inline">\(x^*, \alpha^*, \beta^*\)</span> 分别是原始问题和对偶问题的最优解。</p></blockquote><blockquote><p><strong>定理2:</strong> 考虑原始问题（公式（6））和对偶问题（公式（15））。假设函数 <span class="math inline">\(f(\mathbf{x})\)</span> 和 <span class="math inline">\(c_i(\mathbf{x})\)</span> 是凸函数，<span class="math inline">\(h_j(\mathbf{x})\)</span> 是仿射函数，并且假设不等式约束 <span class="math inline">\(c_i(\mathbf{x})\)</span> 是严格执行的，即存在 <span class="math inline">\(\mathbf{x}\)</span> ，对所有 <span class="math inline">\(i\)</span> 有 <span class="math inline">\(c_i(\mathbf{x}) &lt;0\)</span>， 则存在 <span class="math inline">\(x^*, \alpha^*, \beta^*\)</span>，使 <span class="math inline">\(\mathbf{x}^*\)</span> 是原始问题的解， <span class="math inline">\(\alpha^*\)</span> 和 <span class="math inline">\(\beta^*\)</span> 是对偶问题的解，并且 <span class="math display">\[p^* = d^* = L(\mathbf{x}^*, \alpha^*, \beta^*) \tag{21}\]</span></p></blockquote><blockquote><p><strong>定理3:</strong> 对原始问题（公式（6））和对偶问题（公式（15）），假设函数 <span class="math inline">\(f(\mathbf{x})\)</span> 和 <span class="math inline">\(c_i(\mathbf{x})\)</span> 是凸函数，<span class="math inline">\(h_j(\mathbf{x})\)</span> 是仿射函数，并且假设不等式约束 <span class="math inline">\(c_i(\mathbf{x})\)</span> 是严格执行的， 则存在 <span class="math inline">\(x^*, \alpha^*, \beta^*\)</span> 分别是原始问题和对偶问题的解的充分必要条件是下面的 Karush-Kuhn-Tucker（KKT）条件</p></blockquote><p><span class="math display">\[ \nabla_\mathbf{x} L(\mathbf{x}^*, \alpha^*, \beta^*)=0 \tag{22-1} \]</span> <span class="math display">\[ c_i(\mathbf{x}) \le 0, \quad i =1,2,\ldots,k \tag{22-2}\]</span> <span class="math display">\[ h_j(\mathbf{x}^*) = 0, \quad j=1,2,\ldots, l \tag{22-3}\]</span> <span class="math display">\[\alpha_i^* \ge 0, \quad i=1,2,\ldots,k \tag{22-4}\]</span> <span class="math display">\[ \alpha_i^* c_i(\mathbf{x}^*)=0, \quad i=1,2,\ldots,k \tag{22-5} \]</span> 特别指出，公式（22-5）被称为KKT的对偶互补条件，由此条件可知，如果 <span class="math inline">\(\alpha_i^* &gt;0\)</span>， 则 <span class="math inline">\(c_i(\mathbf{x}^*) = 0\)</span></p><h1 id="kkt条件的解释">4. KKT条件的解释</h1><h2 id="必要性证明">4.1 必要性证明</h2><p>公式（22）为KKT条件，下面对这5个条件逐个进行解释： + 公式（22-1）为广义拉格朗日函数的梯度，表示最优解处的梯度为0. + 公式（22-2）和公式（22-3）分别是愿问题的不等式约束和等式约束，最优解显然应当满足 + 公式（22-4）是对偶问题的不等式约束，表示对偶可行。即当 <span class="math inline">\(\alpha \ge 0\)</span> 时，<span class="math inline">\(L(\mathbf{x}, \alpha, \beta) \le f(\mathbf{x})\)</span>，对偶函数才能给出愿问题的最优值下界。 + 公式（22-5）被称为互补松弛性，推导过程如公式（23）所示： - 第一行：强对偶条件成立，对偶间隙为0 - 第二行：根据公式（13）展开对偶函数 - 第三行：函数的最小值不会超过定义域内任意一点函数值 - 第四行：等式约束 <span class="math inline">\(h_j(\mathbf{x})\)</span> 为0， 而不等式约束 <span class="math inline">\(c_i(\mathbf{x})\le 0\)</span> 且拉格朗日乘子 <span class="math inline">\(\alpha \ge 0\)</span>， 因此成立 <span class="math display">\[\begin{split}f(\mathbf{x}^*) &amp;= \theta_D(\alpha^*, \beta^*) \\&amp;= \min\limits_{\mathbf{x}} \left( f(\mathbf{x}) + \sum_{i=1}^k \alpha_i^* c_i(\mathbf{x}) + \sum_{j=1}^l \beta_j^* h_j(\mathbf{x}) \right) \\&amp; \le f(\mathbf{x^*}) + \sum_{i=1}^k \alpha_i^* c_i(\mathbf{x}) + \sum_{j=1}^l \beta_j^* h_j(\mathbf{x^*}) \\&amp; \le f(\mathbf{x}^*)\end{split}\tag{23}\]</span> - 其中</p><h2 id="充分性证明">4.2 充分性证明</h2><ul><li>公式（22-1）是梯度为0的条件。</li><li>公式（22-2）和公式（22-3）为原问题的不等式约束和等式约束，保证解的可行</li><li>公式（22-4）为对偶可行条件，</li><li>公式（22-5）为互补松弛条件</li></ul><p>所以可以有公式（24）的推断： + 第一行为对偶函数在 <span class="math inline">\((\alpha^*, \beta^*)\)</span> 处的取值 + 第二行为拉格朗日函数的定义 + 第三行是因为互补松弛条件和等式约束</p><p><span class="math display">\[\begin{split}\theta_D(\alpha^*, \beta^*) &amp;= L(\mathbf{x}^*, \alpha^*, \beta^*) \\&amp;= f(\mathbf{x}^*) + \sum_{i=1}^k \alpha_i^* c_i(\mathbf{x}) + \sum_{j=1}^l \beta_j^* h_j(\mathbf{x^*})\\ &amp;= f(\mathbf{x}^*)\end{split}\tag{24}\]</span></p><h1 id="举例">5. 举例</h1><p>考虑如下问题：</p><p><span class="math display">\[\begin{split}\min \quad &amp; x_1^2 + x_2^2 \\\text{s.t.} \quad &amp; x_1 + x_2 = 1\\&amp; x_2 \le \eta\end{split}\tag{25}\]</span> 拉格朗日函数为： <span class="math display">\[L(x_1, x_2, \alpha, \beta) = x_1^2 + x_2^2 + \alpha(x_2 - \eta) + \beta(1 - x_1 -x_2)\tag{26}\]</span> KKT 方程组如下 <span class="math display">\[\begin{split}&amp; \frac{\partial L}{\partial x_i} = 0, \quad i=1,2 \\&amp; x_1 + x_2 = 1\\&amp; x_2 - \eta \le 0 \\&amp; \alpha \ge 0 \\&amp; \alpha(x_2 - \eta) = 0\end{split}\tag{27}\]</span> 对公式（27）求解可得 <span class="math inline">\(\frac{\partial L}{\partial x_1} = 2x_1 - \beta = 0\)</span>, <span class="math inline">\(\frac{\partial L}{\partial x_2} = 2x_2-\beta+\alpha=0\)</span>。分别求解出 <span class="math inline">\(x_1 = \frac{\beta}{2}\)</span>, <span class="math inline">\(x_2 = \frac{\beta}{2} - \frac{\alpha}{2}\)</span>；代入约束等式，可以得到 <span class="math inline">\(x_1 = \frac{\alpha}{4} + \frac{1}{2}\)</span>, <span class="math inline">\(x_2 = -\frac{\alpha}{4} + \frac{1}{2}\)</span>；代入约束不等式 <span class="math inline">\(-\frac{\alpha}{4} + \frac{1}{2} \le \eta\)</span>，以下分三种情况讨论：</p><ul><li><span class="math inline">\(\eta &gt; \frac{1}{2}\)</span>: 可以看出 <span class="math inline">\(\alpha = 0 &gt; 2-4\eta\)</span> 满足所有KKT条件，约束不等式未发挥作用（inactive），<span class="math inline">\(x_1^* = x_2^* = \frac{1}{2}\)</span> 是内部解，目标函数的极小值为 <span class="math inline">\(\frac{1}{2}\)</span></li><li><span class="math inline">\(\eta = \frac{1}{2}\)</span>: <span class="math inline">\(\alpha = 0= 2-4\eta\)</span> 满足所有KKT条件，<span class="math inline">\(x_1^* = x_2^* = \frac{1}{2}\)</span> 是边界解，因此 <span class="math inline">\(x_2^* = \eta\)</span></li><li><span class="math inline">\(\eta &lt; \frac{1}{2}\)</span>: 这时约束不等式是生效的（active），<span class="math inline">\(\alpha = 2-4\eta &gt;0\)</span>，则 <span class="math inline">\(x_1^* = 1-\eta\)</span> 且 <span class="math inline">\(x_2^* = \eta\)</span>， 目标函数极小值是 <span class="math inline">\((1-\alpha)^2 + \alpha^2\)</span></li></ul><h1 id="参考">参考</h1><ol type="1"><li><a href="http://www.tup.tsinghua.edu.cn/booksCenter/book_08132901.html">李航，统计学习方法（第二版）</a></li><li><a href="https://zhuanlan.zhihu.com/p/62420593">支持向量机原理详解(四): KKT条件(Part I) - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/38163970">Karush-Kuhn-Tucker (KKT)条件 - 知乎</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>约束优化</tag>
      
      <tag>最优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Denoising Diffusion Probabilistic Model (DDPM) 论文阅读</title>
    <link href="/2022/10/29/DDPM/"/>
    <url>/2022/10/29/DDPM/</url>
    
    <content type="html"><![CDATA[<h1 id="基本原理介绍">基本原理介绍</h1><p>Diffusion模型和VAE、GAN、流模型等一样都属于生成类模型。Diffusion模型在前向阶段<span class="math inline">\(q(\mathbf{x}_t|\mathbf{x}_{t-1})\)</span>逐渐对图像加噪声，直至图像被完全破坏成高斯噪声，然后在逆向阶段<span class="math inline">\(p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>学习从高斯噪声逐渐还原为原始图像的过程，如图1所示，</p><figure><img src="/2022/10/29/DDPM/DDPM_1.png" alt="图1 DDPM前向与逆向过程"><figcaption aria-hidden="true">图1 DDPM前向与逆向过程</figcaption></figure><h2 id="forward-process-前向阶段">Forward Process （前向阶段）</h2><p>作者认为前向过程中图像<span class="math inline">\(\mathbf{x}_t\)</span>只和上一时刻的<span class="math inline">\(\mathbf{x}_{t-1}\)</span>相关，遵循马尔可夫过程，满足如下性质：</p><p><span class="math display">\[ q(\mathbf{x}_{1:T}|\mathbf{x}_0)=\prod^T_{t=1}q(\mathbf{x}_t | \mathbf{x}_{t-1}) \tag{1}\]</span> <span class="math display">\[ q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbb{I}) \tag{2}\]</span></p><p>其中参数 <span class="math inline">\(\beta_t\)</span> 表示第t时刻高斯分布的方差超参数，并满足 <span class="math inline">\(\beta_1\lt\beta_2\lt\cdots\lt \beta_T\)</span>。公式（2）中 <span class="math inline">\(\sqrt{1-\beta_t}\)</span> 是均值系数。任意时刻可以通过 <strong>重参数技巧</strong> 方法采样得到<span class="math inline">\(\mathbf{x}_t\)</span>。</p><blockquote><p><strong>Reparameterization trick 重参数技巧</strong> 该方法是为了解决随机采样样本这一过程无法求导的问题，例如我们要从某个分布（如高斯分布 <span class="math inline">\(z\sim\mathcal{N}(z;\mu,\sigma^2\mathbb{I})\)</span>）中随机采样一个样本，这个过程无法反传梯度。通常的做法是通过引入随机变量 <span class="math inline">\(\epsilon\sim\mathcal{N}(0,\mathcal{I})\)</span>，使得 <span class="math inline">\(z=\mu+\sigma\odot\epsilon\)</span>。这样一来，<span class="math inline">\(z\)</span> 仍然具有随机性，且服从高斯分布 <span class="math inline">\(\mathcal{N}(\mu, \sigma^2\mathcal{I})\)</span>，同时 <span class="math inline">\(\mu\)</span>与 <span class="math inline">\(\sigma\)</span>可导。</p></blockquote><blockquote><p><strong>正态分布性质</strong> 给定两个服从正态分布的独立随机变量 <span class="math inline">\(X \sim \mathcal{N}(\mu_X, \sigma_X^2)\)</span>, <span class="math inline">\(Y \sim \mathcal{N}(\mu_Y, \sigma^2_Y)\)</span>。这两个分布的加和为 <span class="math inline">\(Z=X+Y\)</span> 同样服从正态分布 <span class="math inline">\(Z \sim \mathcal{N}(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)\)</span>。</p></blockquote><h3 id="根据-mathbfx_0-推断-mathbfx_t">根据 <span class="math inline">\(\mathbf{x}_0\)</span> 推断 <span class="math inline">\(\mathbf{x}_t\)</span></h3><p>根据公式（2）的采样方法，生成随机变量 <span class="math inline">\(\epsilon_t \sim \mathcal{N}(0,\mathbb{I})\)</span>， 然后令 <span class="math inline">\(\alpha_t = 1 - \beta_t\)</span>， 以及 <span class="math inline">\(\overline{\alpha}_t = \prod^T_{i=1} \alpha_i\)</span>， 按照公式（2）中给定的系数，可以做如下推导： <span class="math display">\[\begin{split}\mathbf{x}_t &amp;= \sqrt{1-\beta_t} \mathbf{x}_{t-1} + \beta_t \epsilon_1\\&amp;= \sqrt{\alpha_t} \mathbf{x}_{t-1} + \sqrt{1-\alpha_t} \epsilon_1 \\&amp;= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}\mathbf{x}_{t-2} + \sqrt{1-\alpha_{t-1}} \epsilon_2) + \sqrt{1-\alpha_t} \epsilon_1 \\&amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + (\sqrt{\alpha_t (1-\alpha_{t-1})} \epsilon_2 + \sqrt{1-\alpha_t} \epsilon_1) \\&amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1-\alpha_t \alpha_{t-1}} \tilde{\epsilon_2} \\&amp;= \cdots \\&amp;= \sqrt{\overline{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon_t}\end{split} \tag{3} \]</span> 上式的关键在于第4行到第5行的转换。依赖的正是正态分布性质。<span class="math inline">\(\epsilon_1, \epsilon_2 \sim \mathcal{N}(0, \mathbb{I})\)</span>， 因此，可以做出如下推断： <span class="math display">\[\sqrt{\alpha_t (1 - \alpha_{t-1})} \epsilon_2 \sim \mathcal{N}(0, \alpha_t (1-\alpha_{t-1}) \mathbb{I}) \tag{4}\]</span> <span class="math display">\[\sqrt{1-\alpha_t} \epsilon_1 \sim \mathcal{N}(0, (1-\alpha_t)\mathbb{I}) \tag{5}\]</span> <span class="math display">\[ \begin{split}\sqrt{\alpha_t (1 - \alpha_{t-1})} \epsilon_2 + \sqrt{1-\alpha_t} \epsilon_1 &amp; \sim \mathcal{N}(0, \left[\alpha_t(1-\alpha_{t-1}) + (1-\alpha_t)\right])\\ &amp;= \mathcal{N}(0, (1-\alpha_t \alpha_{t-1})\mathbb{I})\end{split}\tag{6}\]</span> 因此公式（3）可以表示如下： <span class="math display">\[q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t;\sqrt{\overline{\alpha}_t} \mathbf{x}_0, (1-\overline{\alpha}_t) \mathbb{I}) \tag{7}\]</span> 根据前文，参数 <span class="math inline">\(\beta_t \in (0,1)\)</span> 且 <span class="math inline">\(\beta_1 \lt \beta_2 \lt \cdots \lt \beta_T\)</span>，<span class="math inline">\(\alpha_t = 1 - \beta_t\)</span> 且 <span class="math inline">\(\alpha_1 \gt \alpha_2 \gt \cdots \gt \alpha_T\)</span>。由于 <span class="math inline">\(\overline{\alpha}_t = \prod^T_{i=1} \alpha_i\)</span>，因此，当 <span class="math inline">\(T \rightarrow \infty\)</span>，<span class="math inline">\(\overline{\alpha}_t \rightarrow 0\)</span> 且 <span class="math inline">\((1-\overline{\alpha}_t) \rightarrow 1\)</span>，此时，公式（7）趋向于标准正态分布 <span class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(0, \mathbb{I})\)</span>。因此，公式（2）中均值前要乘以系数 <span class="math inline">\(\sqrt{1-\beta_t}\)</span>。</p><h2 id="reverse-process-逆向过程">Reverse Process 逆向过程</h2><p>前向（扩散）过程是给数据添加噪音，逆向过程就是去噪音过程。逆向过程中，我们以高斯噪声 <span class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(0, \mathbb{I})\)</span> 作为输入，如果能够得到逆向过程的分布 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>， 那么我们能够得到真实的样本。根据文献 <a href="https://link.springer.com/chapter/10.1007/978-3-319-16859-3_42">On the theory of stochastic processes, with particular reference to applications</a>，如果 <span class="math inline">\(q(\mathbf{x}_t|\mathbf{x}_{t-1})\)</span> 满足高斯分布且 <span class="math inline">\(\beta_t\)</span> 足够小，<span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>仍然是高斯分布。 由于无法直接推断 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>，因此使用深度学习模型 <span class="math inline">\(p_\theta\)</span> 去拟合 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>，模型参数为 <span class="math inline">\(\theta\)</span>。</p><p><span class="math display">\[p_\theta(\mathbf{x}_{0:T}) = p_\theta(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_T) = p(\mathbf{x}_T) \prod ^T_{t=1} p_\theta (\mathbf{x}_{t-1}|\mathbf{x}_t) \tag{8}\]</span></p><p><span class="math display">\[p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t)) \tag{9}\]</span></p><h3 id="求解条件概率-qmathbfx_t-1mathbfx_t-mathbfx_0">求解条件概率 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)\)</span></h3><p>虽然无法直接求到 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>（注意这里是 <span class="math inline">\(q\)</span> 而不是模型的 <span class="math inline">\(p_\theta\)</span>)，在知道初始分布 <span class="math inline">\(\mathbf{x}_0\)</span> 的情况下，可以通过贝叶斯公式得到 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)\)</span> 为:</p><p><span class="math display">\[q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \textcolor{blue}{\tilde{\mu_t}} (\mathbf{x}_t, \mathbf{x}_0), \textcolor{red} {\tilde{\beta_t}} \mathbb{I}) \tag{10}\]</span></p><blockquote><p><strong>三变量贝叶斯公式</strong> <span class="math display">\[P(x|y,z) = \frac{P(y|x,z)P(x|z)}{P(y|z)} = \frac{P(z|x,y)P(x|y)}{P(z|y)}\]</span></p></blockquote><p>根据上文公式（2）和（7）可知，</p><p><span class="math display">\[q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbb{I}) = \exp \left( -\frac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{2\beta_t} \right)\]</span></p><p><span class="math display">\[q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\overline{\alpha}_t} \mathbf{x}_0, (1-\overline{\alpha}_t) \mathbb{I}) = \exp \left( -\frac{(\mathbf{x}_t - \sqrt{\overline{\alpha}_t} \mathbf{x}_0)^2}{2\beta_t} \right)\]</span></p><p>因此，公式（10）完整推导过程如下： <span class="math display">\[\begin{split}&amp;q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0)\frac{q(\mathbf{x}_{t-1}|\mathbf{x}_0)}{q(\mathbf{x}_t|\mathbf{x}_0)}\\&amp; \propto \exp\left( -\frac{1}{2}\left( \frac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{\beta_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\overline{\alpha}_{t-1}}\mathbf{x}_0)^2}{1 - \overline{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\overline{\alpha}_t}\mathbf{x}_0)^2}{1 - \overline{\alpha}_t} \right)\right) \\&amp;= \exp \left( -\frac{1}{2} \left( \frac{\mathbf{x}_t^2 - 2\sqrt{\alpha_t} \mathbf{x}_t \textcolor{blue}{\mathbf{x}_{t-1}} + \alpha_t \textcolor{red}{\mathbf{x}_{t-1}^2}}{\beta_t} + \frac{\textcolor{red}{\mathbf{x}_{t-1}^2} - 2\sqrt{\overline{\alpha}_{t-1}} \mathbf{x}_0 \textcolor{blue}{\mathbf{x}_{t-1}}  + \overline{\alpha}_{t-1}\mathbf{x}_0^2}{1 - \overline{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\overline{\alpha}_t} \mathbf{x}_0)^2}{1 - \overline{\alpha}_t} \right)\right) \\&amp;= \exp \left( - \frac{1}{2}\left(\textcolor{red}{ (\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \overline{\alpha}_{t-1}})}\mathbf{x}_{t-1}^2 - \textcolor{blue}{(\frac{2\sqrt{\alpha_t}}{\beta_t}\mathbf{x}_t + \frac{2\sqrt{\overline{\alpha}_t - 1}}{1 - \overline{\alpha}_{t-1}} \mathbf{x}_0)} \mathbf{x}_{t-1} + C(\mathbf{x}_t, \mathbf{x}_0)    \right)\right)\end{split} \tag{11}\]</span></p><blockquote><p><strong>高斯概率密度函数</strong> <span class="math display">\[\mathcal{N}(\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma}} \exp \left(-\frac{1}{2} \left( \frac{x-\mu}{\sigma}\right)^2 \right) = \frac{1}{\sqrt{2 \pi \sigma}} \exp \left(-\frac{1}{2} \left(\frac{1}{\sigma^2}x^2 - \frac{2\mu}{\sigma^2} x + \frac{\mu^2}{\sigma^2}\right)\right)\]</span></p></blockquote><p>在上面的推导过程中，我们可以看到通过贝叶斯公式将逆向过程转换为前向过程，且最终得到的概率密度函数和高斯概率密度函数的指数部分能一一对应。结合前文 <span class="math inline">\(\alpha_t + \beta_t = 1\)</span>，我们可以建立如下对应关系：</p><p><span class="math display">\[\begin{split}&amp; \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \overline{\alpha}_{t-1}} = \frac{1}{\tilde{\beta}_t} \\&amp; \Rightarrow \tilde{\beta}_t = \frac{1}{\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \overline{\alpha}_{t-1}}} = \frac{1}{\frac{\alpha_t - \overline{\alpha}_t + \beta_t}{\beta_t(1-\overline{\alpha}_{t-1})}} \\&amp; = \frac{\beta_t(1-\overline{\alpha}_{t-1})}{\alpha_t - \overline{\alpha}_t + \beta_t} = \frac{1 - \overline{\alpha}_{t-1}}{1 - \overline{\alpha}_t} \beta_t\end{split}\tag{12}\]</span></p><p><span class="math display">\[\begin{split}&amp;\frac{2\sqrt{\alpha_t}}{\beta_t}\mathbf{x}_t + \frac{2\sqrt{\overline{\alpha}_t - 1}}{1 - \overline{\alpha}_{t-1}} \mathbf{x}_0 = \frac{2 \tilde{\mu}_t(\mathbf{x}_t, \mathbf{x}_0)}{\tilde{\beta}_t} \\&amp; \Rightarrow \tilde{\mu}_t(\mathbf{x}_t, \mathbf{x}_0) = \left( \frac{\sqrt{\alpha_t}}{\beta_t}\mathbf{x}_t + \frac{\sqrt{\overline{\alpha}_t - 1}}{1 - \overline{\alpha}_{t-1}} \mathbf{x}_0 \right) \tilde{\beta}_t \\&amp;= \left( \frac{\sqrt{\alpha_t}}{\beta_t}\mathbf{x}_t + \frac{\sqrt{\overline{\alpha}_t - 1}}{1 - \overline{\alpha}_{t-1}} \mathbf{x}_0 \right) \frac{1 - \overline{\alpha}_{t-1}}{1 - \overline{\alpha}_t} \beta_t \\&amp;= \frac{(1-\overline{\alpha}_{t-1})\sqrt{\alpha_t} \mathbf{x}_t + \beta_t \sqrt{\overline{\alpha}_{t-1}}\mathbf{x}_0}{\beta_t (1-\overline{\alpha}_{t-1})} \cdot \frac{(1 - \overline{\alpha}_{t-1})\beta_t }{1 - \overline{\alpha}_t} \\&amp;= \frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t} \mathbf{x}_0\end{split}\tag{13}\]</span></p><h3 id="求解均值-mu_thetamathbfx_t-mathbfx_0-和方差-sigma_thetamathbfx_t-mathbfx_0">求解均值 <span class="math inline">\(\mu_\theta(\mathbf{x}_t, \mathbf{x}_0)\)</span> 和方差 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, \mathbf{x}_0)\)</span></h3><p>通过公式（10）和（11），我们可以得到 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)\)</span> 的分布。而且，根据公式（3）<span class="math inline">\(\mathbf{x}_t = \sqrt{\overline{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\overline{\alpha_t}}\tilde{\epsilon}_t\)</span>，可以得到： <span class="math display">\[\mathbf{x}_0 = \frac{1}{\sqrt{\overline{\alpha}_t}}(\mathbf{x}_t - \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon}_t) \tag{14}\]</span></p><p>将公式（14）代入公式（13），可以得到如下结果： <span class="math display">\[\begin{split}\tilde{\mu}_t(\mathbf{x}_t, \mathbf{x}_0) &amp;= \frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t} \mathbf{x}_0 \\&amp;= \frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t} \frac{1}{\sqrt{\overline{\alpha}_t}}(\mathbf{x}_t - \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon}_t) \\&amp;= \frac{\alpha_t(1-\overline{\alpha}_{t-1})\mathbf{x}_t}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)} + \frac{\beta_t}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)}(\mathbf{x}_t - \sqrt{1-\overline{\alpha}_t}\epsilon_t)\\&amp;= \frac{\alpha_t \mathbf{x}_t - \overline{\alpha}_t\mathbf{x}_t + (1-\alpha_t)\mathbf{x}_t - (1-\alpha_t)\sqrt{1-\overline{\alpha}_t}\epsilon_t}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)} \\&amp;= \frac{(1-\overline{\alpha}_t)\mathbf{x}_t - (1-\alpha_t)\sqrt{1-\overline{\alpha}_t}\epsilon_t}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)}\\&amp;= \frac{\mathbf{x}_t}{\sqrt{\alpha_t}} - \frac{(1-\alpha_t)\epsilon_t}{\sqrt{\alpha_t}\sqrt{1-\overline{\alpha}_t}}\\&amp;= \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_t \right)\end{split}\tag{15}\]</span></p><p>之前说到，我们使用深度学习模型 <span class="math inline">\(p_\theta\)</span> 去拟合逆向过程的分布 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>， 根据公式（9）可知，<span class="math inline">\(p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t))\)</span>，我们希望训练模型 <span class="math inline">\(\mu_\theta(\mathbf{x}_t, t)\)</span> 以预估 <span class="math inline">\(\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_t \right)\)</span>。由于 <span class="math inline">\(\mathbf{x}_t\)</span> 在训练阶段的逆向过程中是输入的图片数据，因此是已知的，我们可以转而让模型去预估噪声 <span class="math inline">\(\epsilon_t\)</span>， 即令：</p><p><span class="math display">\[\mu_\theta (\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_\theta(\mathbf{x}_t, t) \right) \tag{16}\]</span></p><p>因此，</p><p><span class="math display">\[ \mathbf{x}_{t-1} = \mathcal{N}(\mathbf{x}_{t-1}; \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_\theta(\mathbf{x}_t, t) \right), \Sigma_\theta(\mathbf{x}_t, t) ) \tag{17}\]</span></p><p>方差 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, t)\)</span> 可以有多种选择，DDPM使用的是 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, t) = \tilde{\beta}_t\)</span> 且认为 <span class="math inline">\(\tilde{\beta}_t = \beta_t\)</span> 和 <span class="math inline">\(\tilde{\beta}_t = \frac{1 - \overline{\alpha}_{t-1}}{1 - \overline{\alpha}_t} \cdot \beta_t\)</span> 的结果近似，而在<a href="https://arxiv.org/abs/2112.10741">GLIDE</a>中，则是根据网络预测的结果计算方差。</p><h3 id="ddpm-推断过程">DDPM 推断过程</h3><p>因此，DDPM每一步推断可以总结如下：</p><ul><li>每个时间步骤通过 <span class="math inline">\(\mathbf{x}_t\)</span> 和步骤 <span class="math inline">\(t\)</span> 来预测高斯噪声 <span class="math inline">\(\epsilon_\theta(\mathbf{x}_t, t)\)</span>，然后根据公式（16）得到均值 <span class="math inline">\(\mu_\theta(\mathbf{x}_t, t)\)</span></li><li>得到方差 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, t)\)</span>， DDPM使用的是 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, t) = \tilde{\beta}_t\)</span> 且认为 <span class="math inline">\(\tilde{\beta}_t = \beta_t\)</span> 和 <span class="math inline">\(\tilde{\beta}_t = \frac{1 - \overline{\alpha}_{t-1}}{1 - \overline{\alpha}_t} \cdot \beta_t\)</span> 的结果近似，</li><li>根据公式（9）得到 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>，利用重参数得到 <span class="math inline">\(\mathbf{x}_{t-1}\)</span></li></ul><figure><img src="/2022/10/29/DDPM/DDPM_Forw_Rev.png" alt="图2 DDDPM前向与逆向数据转换过程，可以看到逆向过程中 \mathbf{x}_0 和 \mathbf{x}_t 之间反复横跳"><figcaption aria-hidden="true">图2 DDDPM前向与逆向数据转换过程，可以看到逆向过程中 <span class="math inline">\(\mathbf{x}_0\)</span> 和 <span class="math inline">\(\mathbf{x}_t\)</span> 之间反复横跳</figcaption></figure><h2 id="diffusion-模型训练">Diffusion 模型训练</h2><p>逆向阶段是让模型去预估噪声 <span class="math inline">\(\epsilon_\theta(\mathbf{x}_t, t)\)</span>，从而产生较好的 <span class="math inline">\(\mu_\theta(\mathbf{x}_t, t)\)</span> 和 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, t)\)</span>， 以求得概率 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>。 作者的目标是在真实数据分布下，最大化模型预测分布的对数似然函数，即优化 <span class="math inline">\(\mathcal{x}_0 \sim q(\mathcal{x}_0)\)</span> 下的 <span class="math inline">\(p_\theta(\mathcal{x}_0)\)</span> 的交叉熵。</p><p><span class="math display">\[\mathcal{L} =\mathbb{E}_{q(\mathbf{x}_0)} \left[ -\log p_\theta(\mathcal{x}_0) \right] \tag{18} \]</span></p><p>和变分自动编码器类似，使用变分下限（Variantional Lower Bound, VLB）也称ELBO（Evidence Lower Bound），来优化 <span class="math inline">\(-\log p_\theta(\mathbf{x}_0)\)</span>：</p><p><span class="math display">\[\begin{split}-\log p_\theta(\mathbf{x}_0) &amp;\le -\log p_\theta(\mathbf{x}_0) + KL(q(\mathbf{x}_{1:T}|\mathbf{x}_0)||p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)) \\&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \left[\log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] \\&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \left[\log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})/p_\theta(\mathbf{x}_0)}\right] \\&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \left[\log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} + \underbrace{\log p_\theta(\mathbf{x}_0)}_{与q无关}\right] \\&amp;= \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \left[\log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \right] \\\end{split}\tag{19}\]</span></p><blockquote><p><strong>KL散度</strong> <span class="math display">\[KL(P||Q) = \sum_i P(x_i) \log \frac{P(x_i)}{Q(x_i)}\]</span></p><p><strong>Fubini定理</strong></p><p>如果 <span class="math inline">\(\int_{A\times B} |f(x,y)| d(x,y) &lt; \infty\)</span>, 则下式成立 <span class="math display">\[\int_A \left(\int_B f(x,y) dy\right)dx = \int_B \left(\int_A f(x,y) dx\right)dy = \int_{A\times B} f(x,y) d(x,y)\]</span></p></blockquote><h3 id="fubini定理推断mathcall_vlb">Fubini定理推断<span class="math inline">\(\mathcal{L}_{VLB}\)</span></h3><p>根据公式（18），对公式（19）左右两边取期望 <span class="math inline">\(\mathbb{E}_{q(\mathbf{x}_0)}\)</span>，利用重积分中的Fubini定理： <span class="math display">\[\begin{split}\mathcal{L}_{VLB} &amp;= \mathbb{E}_{q(\mathbf{x}_0)}\left( \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \left[\log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \right]  \right) = \mathbb{E}_{q(\mathbf{x_{0:T}})} \left[ \log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})}  \right] \\&amp; \ge \mathbb{E}_{q(\mathbf{x}_0)} \left[ -\log p_\theta(\mathcal{x}_0) \right]\end{split}\tag{20}\]</span></p><p>最小化 <span class="math inline">\(\mathcal{L}_{VLB}\)</span> 即可最小化我们的目标损失函数，即公式（18）。</p><h3 id="jensen不等式推断mathcall_vlb">Jensen不等式推断<span class="math inline">\(\mathcal{L}_{VLB}\)</span></h3><blockquote><p><strong>Jensen不等式概率论版本</strong></p><p>对于随机变量<span class="math inline">\(X\)</span>，<span class="math inline">\(\varphi\)</span> 是任意凸函数，则下式成立 <span class="math display">\[\varphi(E(X)) \le E(\varphi(X))\]</span></p></blockquote><p><span class="math display">\[\begin{split}\mathcal{L} &amp;= \mathbb{E}_{q(\mathbf{x}_0)} \left[ -\log p_\theta(\mathcal{x}_0) \right] \\&amp;= - \mathbb{E}_{q(\mathbf{x}_0)} \log \left( p_\theta(\mathbf{x}_0) \cdot \int p_\theta(\mathbf{x}_{1:T}) d\mathbf{x}_{1:T} \right) \\&amp;= - \mathbb{E}_{q(\mathbf{x}_0)}\log\left( \int p_\theta(\mathbf{x}_{0:T})d\mathbf{x}_{1:T} \right)\\&amp;= - \mathbb{E}_{q(\mathbf{x}_0)}\log\left( \int q(\mathbf{x}_{1:T}|\mathbf{x}_0) \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} d\mathbf{x}_{1:T} \right)\\&amp;= - \mathbb{E}_{q(\mathbf{x}_0)}\log\left( \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \right)\\&amp;= \mathbb{E}_{q(\mathbf{x}_0)}\log\left( \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \right)\\&amp;\le \mathbb{E}_{q(\mathbf{x}_{0:T})} \log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} = \mathcal{L}_{VLB}\end{split} \tag{21}\]</span></p><p>可以看到，通过Fubini定理和Jensen不等式都可以得到相同的结果。这里需要解释下公式（21）中几个步骤。</p><ul><li>第2行到第3行：因为 <span class="math inline">\(p_\theta(\mathbf{x}_0)\)</span> 和 <span class="math inline">\(\mathbf{x}_{1:T}\)</span> 无关，因此可以将 <span class="math inline">\(p_\theta(\mathbf{x}_0)\)</span> 看作常数项与 <span class="math inline">\(p_\theta(\mathbf{x}_{1:T})\)</span> 相乘。根据联合概率密度函数定义，我们可以看出 <span class="math inline">\(p_\theta(\mathbf{x}_{0:T}) = p_\theta(\mathbf{x}_0) p_\theta(\mathbf{x}_{1:T})\)</span></li><li>第6行到第7行：将 <span class="math inline">\(\log\)</span> 函数看作 <span class="math inline">\(\varphi\)</span>，根据Jensen不等式可知不等号成立。且根据 <span class="math inline">\(p_\theta(\mathbf{x}_{0:T}) = p_\theta(\mathbf{x}_0) p_\theta(\mathbf{x}_{1:T})\)</span>，我们可以看出，对 <span class="math inline">\(q(\mathbf{x}_0)\)</span> 和 <span class="math inline">\(q(\mathbf{x}_{1:T}|\mathbf{x}_0)\)</span> 连续积分可以得到 <span class="math inline">\(q(\mathbf{x}_{0:T})\)</span></li></ul><h3 id="进一步拆解-_vlb">进一步拆解 $_{VLB} $</h3><p>进一步对 <span class="math inline">\(\mathcal{L}_{VLB}\)</span> 推导，可以得到熵与多个KL散度的累加，具体推导如下：</p><p><span class="math display">\[\begin{split}\mathcal{L}_{VLB} &amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \log \frac{q(\mathcal{x}_{1:T}|\mathcal{x}_0)}{p_\theta(\mathcal{x}_{0:T})} \right]\\&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \log \frac{\prod^T_{t=1} q(\mathbf{x}_t|\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_T)\prod^T_{t=1}p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} \right]\\&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=1} \log \frac{q(\mathbf{x}_t|\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} \right]\\&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} + \log \frac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)} \right]\\&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=2} \log \left( \frac{q(\mathbf{x}_t|\mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} \cdot \frac{q(\mathbf{x}_t|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)} \right) + \log \frac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)} \right]\\&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)} \right]\\&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} + \log \frac{q(\mathbf{x}_T|\mathbf{x}_0)}{q(\mathbf{x}_{1}|\mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)} \right]\\&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}  + \log q(\mathbf{x}_1|\mathbf{x}_0) - \log p_\theta(\mathbf{x}_0|\mathbf{x}_1) \right] \\&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \log \frac{q(\mathbf{x}_T|\mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} - \log p_\theta(\mathbf{x}_0|\mathbf{x}_1) \right]\\&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \underbrace{KL(q(\mathbf{x}_T|\mathbf{x}_0)||p_\theta(\mathbf{x}_T))}_{L_T} + \sum^T_{t=2} \underbrace{KL(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)||p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))}_{L_{t-1}} - \underbrace{\log p_\theta(\mathbf{x}_0|\mathbf{x}_1)}_{L_0} \right]\end{split}\tag{22}\]</span></p><p>公式（22）的解释： + 第1行到第2行：分子是根据公式（1）得来。分母是根据公式（8）得来 + 第4行到第5行：我们可以看到，第四行中 <span class="math inline">\(\log\)</span> 符号后分式中分母保持不变，只是分子 <span class="math inline">\(q(\mathbf{x}_t|\mathbf{x}_{t-1})\)</span> 通过贝叶斯公式转换： <span class="math display">\[q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \frac{q(\mathbf{x}_t, \mathbf{x}_{t-1}|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)} = \frac{q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_0) \cdot p(\mathbf{x}_{t}|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)} \]</span></p><p>公式（22）可以重新写为以下公式： <span class="math display">\[\mathcal{L}_{VLB} = L_T + L_{T-1} + \cdots + L_0 \tag{23-1}\]</span> <span class="math display">\[L_T = KL(q(\mathbf{x}_T|\mathbf{x}_0)||p_\theta(\mathbf{x}_T)) \tag{23-2}\]</span> <span class="math display">\[L_t = KL(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)||p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)); \quad 1 \le t \le T-1 \tag{23-3}\]</span> <span class="math display">\[L_0 = - \log p_\theta(\mathbf{x}_0|\mathbf{x}_1) \tag{23-4}\]</span></p><h3 id="ddpm的loss">DDPM的LOSS</h3><p>由于前向过程 <span class="math inline">\(q\)</span> 没有可学习的参数，而且 <span class="math inline">\(\mathcal{x}_T\)</span> 是纯高斯噪声，因此公式（23-2）中的 <span class="math inline">\(L_T\)</span> 可以当作常量忽略。最后一项 <span class="math inline">\(L_0\)</span> 作者使用离散分段累计函数来计算，可以避免计算协方差等过程，但该项对最终结果影响不大。 <span class="math inline">\(L_t\)</span> 可以看作是拉近两个变量的高斯分布，即公式（10）<span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \textcolor{blue}{\tilde{\mu_t}} (\mathbf{x}_t, \mathbf{x}_0), \textcolor{red} {\tilde{\beta_t}} \mathbb{I})\)</span> 和公式（9）<span class="math inline">\(p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t))\)</span>，根据<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions">多元高斯分布的KL散度求解</a>，可以得到下式： <span class="math display">\[L_t = \mathbb{E}_{q(\mathbf{x}_{0:T})}\left[\frac{1}{2||\Sigma_\theta(\mathbf{x}_t, \mathbf{x}_0)||_2^2}||\tilde{\mu}(\mathbf{x}_t, \mathbf{x}_0) - \mu_\theta(\mathbf{x}_t, t)||^2 \right]+C_t \tag{24}\]</span></p><blockquote><p><strong>多元高斯分布KL散度解释</strong></p><p><span class="math display">\[KL(p||q) = \frac{(\mu_1 - \mu_2)^2}{2 \sigma_2^2} + \frac{\sigma_1^2}{2\sigma_2^2} + \log \frac{\sigma_2}{\sigma_p} - \frac{1}{2} \]</span> 由于DDPM中只有均值是与参数 <span class="math inline">\(\theta\)</span> 相关，因此，这里后三项都是与参数 <span class="math inline">\(\theta\)</span> 无关的常数。</p></blockquote><p>其中公式（24）中参数 <span class="math inline">\(C\)</span> 是与模型参数 <span class="math inline">\(\theta\)</span> 无关的常量。将公式（15）和公式（16）的结果代入公式（24）可以得到： <span class="math display">\[\begin{split}L_t &amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon}\left[\frac{1}{2||\Sigma_\theta(\mathbf{x}_t, \mathbf{x}_0)||_2^2}||\tilde{\mu}(\mathbf{x}_t, \mathbf{x}_0) - \mu_\theta(\mathbf{x}_t, t)||^2 \right]\\ &amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon}\left[\frac{1}{2||\Sigma_\theta||_2^2}\left|\left|\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_t \right) - \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_\theta(\mathbf{x}_t, t) \right)\right|\right|^2 \right]\\&amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ \frac{1}{2||\Sigma_\theta||_2^2} \left|\left| \frac{1}{\sqrt{\overline{\alpha}_t}} \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} (\epsilon_t - \epsilon_\theta(\mathbf{x}_t, t)) \right|\right|^2\right] \\&amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ \frac{(1-\alpha_t)^2}{2 \alpha_t (1-\overline{\alpha}_t) ||\Sigma_\theta||^2_2} ||\epsilon_t - \epsilon_\theta(\mathbf{x}_t, t)||^2 \right] \\&amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ \frac{(1-\alpha_t)^2}{2 \alpha_t (1-\overline{\alpha}_t) ||\Sigma_\theta||^2_2} ||\epsilon_t - \epsilon_\theta(\sqrt{\overline{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon_t}, t)||^2 \right] \\\end{split}\tag{25}\]</span></p><p>公式（25）中最后一行是根据公式（3）将 <span class="math inline">\(\mathbf{x}_t\)</span> 替换为 <span class="math inline">\(\mathbf{x}_0\)</span>。因此DDPM的Loss狠心是学习高斯噪声 <span class="math inline">\(\epsilon_t\)</span> 和 <span class="math inline">\(\epsilon_\theta\)</span> 之间的MSE。</p><h2 id="ddpm最终算法">DDPM最终算法</h2><p>DDPM最终算法流程如下，其中Training表示训练阶段，Sampling表示逆向阶段。根据代码理解，Train完成之后，通过Sampling生成图像。 <img src="/2022/10/29/DDPM/DDPM_Total_Alg.png" alt="DDPM最终算法"></p><p><strong>训练阶段的步骤：</strong></p><ul><li><p>从数据集中采样 <span class="math inline">\(\mathbf{x}_0\)</span></p></li><li><p>随机选取时间步骤<span class="math inline">\(t\)</span></p></li><li><p>生成高斯噪声 <span class="math inline">\(\epsilon_t \in \mathcal{N}(0, \mathbb{I})\)</span></p></li><li><p>预估 <span class="math inline">\(\epsilon_\theta(\sqrt{\overline{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon_t}, t)\)</span></p></li><li><p>计算MSE Loss: <span class="math inline">\(||\epsilon_t - \epsilon_\theta(\sqrt{\overline{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon_t}, t)||^2\)</span>，并利用反向传播算法训练模型</p></li></ul><p><strong>逆向阶段采用如下步骤进行采样：</strong></p><ul><li><p>从高斯分布中采样 <span class="math inline">\(\mathcal{x}_T\)</span></p></li><li><p>按照 <span class="math inline">\(T,T-1,\cdots,1\)</span>的顺序进行迭代</p><ul><li><p>如果 <span class="math inline">\(t\)</span> = 1，令 <span class="math inline">\(z=0\)</span>；如果 <span class="math inline">\(t&gt;1\)</span>，则从高斯分布中采样 <span class="math inline">\(z\sim\mathcal{N}(0, \mathbb{I})\)</span></p></li><li><p>利用公式（16）学习出均值 <span class="math inline">\(\mu_\theta(\mathbf{x}_t, t)\)</span>，并利用公式（12）计算方差 <span class="math inline">\(\sigma_t\)</span></p></li><li><p>通过重参数技巧采样 <span class="math inline">\(\mathbf{x}_{t-1} = \mu_\theta(\mathbf{x}_t, t) + \sigma_t z\)</span></p></li></ul></li><li><p>通过以上步骤恢复出 <span class="math inline">\(\mathbf{x}_0\)</span></p></li></ul><h1 id="代码解读">代码解读</h1><p>Coming soon</p><h1 id="ddim加速diffusion采样和方差的选择">DDIM：加速Diffusion采样和方差的选择</h1><p>DDPM的高质量生成依赖较大的 <span class="math inline">\(T\)</span>, 这就导致Diffusion的前向过程非常缓慢，因此有作者提出一种牺牲多样性来换取更快推断的手段，提出了 <a href="https://arxiv.org/abs/2010.02502">Denoising diffusion implicit model (DDIM)</a>。</p><p>根据公式（3）及高斯分布可加性，可以得到 <span class="math inline">\(\mathbf{x}_{t-1}\)</span> 为： <span class="math display">\[\begin{split}\mathbf{x}_{t-1} &amp;= \sqrt{\overline{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1 - \overline{\alpha}_{t-1}} \tilde{\epsilon}_{t-1} \\&amp;= \sqrt{\overline{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_{t-1}-\sigma^2_t}\tilde{\epsilon}_t + \sigma_t \epsilon_t \\&amp;= \sqrt{\overline{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1 - \overline{\alpha}_{t-1} - \sigma^2_t} \left( \frac{\mathbf{x}_t - \sqrt{\overline{\alpha}_t}\mathbf{x}_0}{\sqrt{1-\overline{\alpha}_t}} \right) + \sigma_t \epsilon_t\end{split}\tag{26}\]</span></p><p>根据上文，我们可以得知 <span class="math inline">\(\epsilon_t \sim \mathcal{N}(0,1)\)</span> 是服从标准正态分布的噪声, <span class="math inline">\(\tilde{\epsilon}_t \sim \mathcal{N}(0,1)\)</span> 是根据正态分布可加性变换过的且服从标准正态分布的噪声。 公式（26）第二行是根据高斯分布可加性推导而来， 假定 $P_A =  <em>{t-1} (0, (1-</em>{t-1}) ) <span class="math inline">\(，\)</span>P_B = (0, _t^2) $，我们可以得到如下推断： <span class="math display">\[\begin{split}P_A &amp;= P_A - P_B + P_B\\&amp;= \sqrt{\overline{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_{t-1}-\sigma^2_t}\tilde{\epsilon}_t + \sigma_t \epsilon_t \\\end{split}\tag{27}\]</span> 公式（26）第二行到第三行也是根据公式（3）变换而来： <span class="math display">\[\begin{split}&amp; \mathbf{x}_{t} = \sqrt{\overline{\alpha}_{t}} \mathbf{x}_0 + \sqrt{1 - \overline{\alpha}_{t}} \tilde{\epsilon}_{t} \\ &amp; \Rightarrow \tilde{\epsilon}_t = \frac{\mathbf{x}_t - \sqrt{\overline{\alpha}_t}\mathbf{x}_0}{\sqrt{1-\overline{\alpha}_t}}\end{split}\tag{28}\]</span> 因此可以重写公式（10）和（11）为 <span class="math display">\[q_\sigma (\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\overline{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1 - \overline{\alpha}_{t-1} - \sigma^2_t} \left( \frac{\mathbf{x}_t - \sqrt{\overline{\alpha}_t}\mathbf{x}_0}{\sqrt{1-\overline{\alpha}_t}} \right), \sigma_t^2 \mathbb{I}) \tag{29}\]</span></p><p>不同于公式（10）和（16），公式（26）将方差 <span class="math inline">\(\sigma_t^2\)</span> 引入到了均值当中，当 <span class="math inline">\(\sigma_t^2 = \tilde{\beta}_t = \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t} \beta_t\)</span> 时，公式（26）等价于公式（10）。 在DDIM中将由公式（26）得到的 <span class="math inline">\(q_\sigma (\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)\)</span> 称为非马尔可夫过程，因为 <span class="math inline">\(\mathbf{x}_t\)</span> 的分布同时依赖 <span class="math inline">\(\mathbf{x}_{t-1}\)</span> 和 <span class="math inline">\(\mathbf{x}_0\)</span>。DDIM进一步定义了 <span class="math inline">\(\sigma_t(\eta)^2 = \eta \cdot \tilde{\beta}_t\)</span>。当 <span class="math inline">\(\eta=0\)</span> 时，diffusion的采样过程会丧失所有随机性从而得到一个确定性的结果，但是可以改变 <span class="math inline">\(\mathbf{x}_T\)</span>。而当 <span class="math inline">\(\eta = 1\)</span> 时，DDIM等驾驭DDPM （使用 <span class="math inline">\(\tilde{\beta}_t\)</span>作为方差的版本），用随机性换取生成性能。</p><p>对于方差 <span class="math inline">\(\sigma_t^2\)</span> 的选择，可以总结如下：</p><ul><li>DDPM:<ul><li><span class="math inline">\(\sigma^2_{t,\theta} = \Sigma_\theta(\mathbf{x}_t, t)\)</span> 相当于模型学习的方差，DDPM称为learned，实际没有使用，GLIDE使用了这种方差。</li><li><span class="math inline">\(\sigma^2_{t,s} = \tilde{\beta}_t = \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t} \beta_t\)</span>，DDPM称为fixedsamll，用于CelebA-HQ数据集和LSUN。</li><li><span class="math inline">\(\sigma^2_{t,l} = \beta_t\)</span>，DDPM称之为fixedlarge，用于CIFAR10数据集，注意 <span class="math inline">\(\sigma_{t,l}&gt;\sigma_{t,s}\)</span></li></ul></li><li>DDIM:<ul><li><span class="math inline">\(\sigma_t(\eta)^2 = \eta \cdot \tilde{\beta}_t\)</span>，DDIM是在fixedsmall版本上再乘以一个系数 <span class="math inline">\(\eta\)</span>。</li></ul></li></ul><p>假设总的采样步骤是 <span class="math inline">\(T\)</span>, 采样间隔是 <span class="math inline">\(Q\)</span>，DDIM的采样步数为 <span class="math inline">\(S=T/Q\)</span>，<span class="math inline">\(S\)</span> 和 <span class="math inline">\(\eta\)</span> 的实验结果如下： <img src="/2022/10/29/DDPM/DDIM_Exp.png" alt="DDIM的实验结果"> 可以看到在 <span class="math inline">\(S\)</span> 很小的时候 <span class="math inline">\(\eta = 0\)</span> 取得了最好的结果，而当步骤 <span class="math inline">\(T\)</span> 足够大的时候，使用更大的方差 <span class="math inline">\(\sigma_t^2\)</span> 可以得到更好的结果。</p><h1 id="参考">参考</h1><ol type="1"><li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models?</a></li><li><a href="https://zhuanlan.zhihu.com/p/525106459">由浅入深了解Diffusion Model - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/576475987">扩散模型（Diffusion Model）简要介绍于源码分析</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经网络</tag>
      
      <tag>生成模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/10/29/hello-world/"/>
    <url>/2022/10/29/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>

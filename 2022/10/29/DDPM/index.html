

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Huacheng Li">
  <meta name="keywords" content="">
  
    <meta name="description" content="基本原理介绍 Diffusion模型和VAE、GAN、流模型等一样都属于生成类模型。Diffusion模型在前向阶段\(q(\mathbf{x}_t|\mathbf{x}_{t-1})\)逐渐对图像加噪声，直至图像被完全破坏成高斯噪声，然后在逆向阶段\(p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)\)学习从高斯噪声逐渐还原为原始图像的过程，如图1所示，  图1">
<meta property="og:type" content="article">
<meta property="og:title" content="Denoising Diffusion Probabilistic Model (DDPM) 论文阅读">
<meta property="og:url" content="http://watsonlee.github.io/2022/10/29/DDPM/index.html">
<meta property="og:site_name" content="努力减肥的小李">
<meta property="og:description" content="基本原理介绍 Diffusion模型和VAE、GAN、流模型等一样都属于生成类模型。Diffusion模型在前向阶段\(q(\mathbf{x}_t|\mathbf{x}_{t-1})\)逐渐对图像加噪声，直至图像被完全破坏成高斯噪声，然后在逆向阶段\(p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)\)学习从高斯噪声逐渐还原为原始图像的过程，如图1所示，  图1">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://watsonlee.github.io/2022/10/29/DDPM/DDPM_1.png">
<meta property="og:image" content="http://watsonlee.github.io/2022/10/29/DDPM/DDPM_Forw_Rev.png">
<meta property="og:image" content="http://watsonlee.github.io/2022/10/29/DDPM/DDPM_Total_Alg.png">
<meta property="og:image" content="http://watsonlee.github.io/2022/10/29/DDPM/DDIM_Exp.png">
<meta property="article:published_time" content="2022-10-29T12:05:07.000Z">
<meta property="article:modified_time" content="2022-11-15T05:02:41.000Z">
<meta property="article:author" content="Huacheng Li">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="生成模型">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://watsonlee.github.io/2022/10/29/DDPM/DDPM_1.png">
  
  
  
  <title>Denoising Diffusion Probabilistic Model (DDPM) 论文阅读 - 努力减肥的小李</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"watsonlee.github.io","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":6},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Watsonlee</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/hanta1.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Denoising Diffusion Probabilistic Model (DDPM) 论文阅读"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-10-29 20:05" pubdate>
          October 29, 2022 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          24k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          204 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Denoising Diffusion Probabilistic Model (DDPM) 论文阅读</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="基本原理介绍">基本原理介绍</h1>
<p>Diffusion模型和VAE、GAN、流模型等一样都属于生成类模型。Diffusion模型在前向阶段<span class="math inline">\(q(\mathbf{x}_t|\mathbf{x}_{t-1})\)</span>逐渐对图像加噪声，直至图像被完全破坏成高斯噪声，然后在逆向阶段<span class="math inline">\(p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>学习从高斯噪声逐渐还原为原始图像的过程，如图1所示，</p>
<figure>
<img src="/2022/10/29/DDPM/DDPM_1.png" srcset="/img/loading.gif" lazyload alt="图1 DDPM前向与逆向过程"><figcaption aria-hidden="true">图1 DDPM前向与逆向过程</figcaption>
</figure>
<h2 id="forward-process-前向阶段">Forward Process （前向阶段）</h2>
<p>作者认为前向过程中图像<span class="math inline">\(\mathbf{x}_t\)</span>只和上一时刻的<span class="math inline">\(\mathbf{x}_{t-1}\)</span>相关，遵循马尔可夫过程，满足如下性质：</p>
<p><span class="math display">\[ q(\mathbf{x}_{1:T}|\mathbf{x}_0)=\prod^T_{t=1}q(\mathbf{x}_t | \mathbf{x}_{t-1}) \tag{1}\]</span> <span class="math display">\[ q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbb{I}) \tag{2}\]</span></p>
<p>其中参数 <span class="math inline">\(\beta_t\)</span> 表示第t时刻高斯分布的方差超参数，并满足 <span class="math inline">\(\beta_1\lt\beta_2\lt\cdots\lt \beta_T\)</span>。公式（2）中 <span class="math inline">\(\sqrt{1-\beta_t}\)</span> 是均值系数。任意时刻可以通过 <strong>重参数技巧</strong> 方法采样得到<span class="math inline">\(\mathbf{x}_t\)</span>。</p>
<blockquote>
<p><strong>Reparameterization trick 重参数技巧</strong> 该方法是为了解决随机采样样本这一过程无法求导的问题，例如我们要从某个分布（如高斯分布 <span class="math inline">\(z\sim\mathcal{N}(z;\mu,\sigma^2\mathbb{I})\)</span>）中随机采样一个样本，这个过程无法反传梯度。通常的做法是通过引入随机变量 <span class="math inline">\(\epsilon\sim\mathcal{N}(0,\mathcal{I})\)</span>，使得 <span class="math inline">\(z=\mu+\sigma\odot\epsilon\)</span>。这样一来，<span class="math inline">\(z\)</span> 仍然具有随机性，且服从高斯分布 <span class="math inline">\(\mathcal{N}(\mu, \sigma^2\mathcal{I})\)</span>，同时 <span class="math inline">\(\mu\)</span>与 <span class="math inline">\(\sigma\)</span>可导。</p>
</blockquote>
<blockquote>
<p><strong>正态分布性质</strong> 给定两个服从正态分布的独立随机变量 <span class="math inline">\(X \sim \mathcal{N}(\mu_X, \sigma_X^2)\)</span>, <span class="math inline">\(Y \sim \mathcal{N}(\mu_Y, \sigma^2_Y)\)</span>。这两个分布的加和为 <span class="math inline">\(Z=X+Y\)</span> 同样服从正态分布 <span class="math inline">\(Z \sim \mathcal{N}(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)\)</span>。</p>
</blockquote>
<h3 id="根据-mathbfx_0-推断-mathbfx_t">根据 <span class="math inline">\(\mathbf{x}_0\)</span> 推断 <span class="math inline">\(\mathbf{x}_t\)</span></h3>
<p>根据公式（2）的采样方法，生成随机变量 <span class="math inline">\(\epsilon_t \sim \mathcal{N}(0,\mathbb{I})\)</span>， 然后令 <span class="math inline">\(\alpha_t = 1 - \beta_t\)</span>， 以及 <span class="math inline">\(\overline{\alpha}_t = \prod^T_{i=1} \alpha_i\)</span>， 按照公式（2）中给定的系数，可以做如下推导： <span class="math display">\[\begin{split}
\mathbf{x}_t &amp;= \sqrt{1-\beta_t} \mathbf{x}_{t-1} + \beta_t \epsilon_1\\
&amp;= \sqrt{\alpha_t} \mathbf{x}_{t-1} + \sqrt{1-\alpha_t} \epsilon_1 \\
&amp;= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}\mathbf{x}_{t-2} + \sqrt{1-\alpha_{t-1}} \epsilon_2) + \sqrt{1-\alpha_t} \epsilon_1 \\
&amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + (\sqrt{\alpha_t (1-\alpha_{t-1})} \epsilon_2 + \sqrt{1-\alpha_t} \epsilon_1) \\
&amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1-\alpha_t \alpha_{t-1}} \tilde{\epsilon_2} \\
&amp;= \cdots \\
&amp;= \sqrt{\overline{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon_t}
\end{split} \tag{3} \]</span> 上式的关键在于第4行到第5行的转换。依赖的正是正态分布性质。<span class="math inline">\(\epsilon_1, \epsilon_2 \sim \mathcal{N}(0, \mathbb{I})\)</span>， 因此，可以做出如下推断： <span class="math display">\[\sqrt{\alpha_t (1 - \alpha_{t-1})} \epsilon_2 \sim \mathcal{N}(0, \alpha_t (1-\alpha_{t-1}) \mathbb{I}) \tag{4}\]</span> <span class="math display">\[\sqrt{1-\alpha_t} \epsilon_1 \sim \mathcal{N}(0, (1-\alpha_t)\mathbb{I}) \tag{5}\]</span> <span class="math display">\[ \begin{split}\sqrt{\alpha_t (1 - \alpha_{t-1})} \epsilon_2 + \sqrt{1-\alpha_t} \epsilon_1 &amp; \sim \mathcal{N}(0, \left[\alpha_t(1-\alpha_{t-1}) + (1-\alpha_t)\right])\\ &amp;= \mathcal{N}(0, (1-\alpha_t \alpha_{t-1})\mathbb{I})\end{split}\tag{6}\]</span> 因此公式（3）可以表示如下： <span class="math display">\[q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t;\sqrt{\overline{\alpha}_t} \mathbf{x}_0, (1-\overline{\alpha}_t) \mathbb{I}) \tag{7}\]</span> 根据前文，参数 <span class="math inline">\(\beta_t \in (0,1)\)</span> 且 <span class="math inline">\(\beta_1 \lt \beta_2 \lt \cdots \lt \beta_T\)</span>，<span class="math inline">\(\alpha_t = 1 - \beta_t\)</span> 且 <span class="math inline">\(\alpha_1 \gt \alpha_2 \gt \cdots \gt \alpha_T\)</span>。由于 <span class="math inline">\(\overline{\alpha}_t = \prod^T_{i=1} \alpha_i\)</span>，因此，当 <span class="math inline">\(T \rightarrow \infty\)</span>，<span class="math inline">\(\overline{\alpha}_t \rightarrow 0\)</span> 且 <span class="math inline">\((1-\overline{\alpha}_t) \rightarrow 1\)</span>，此时，公式（7）趋向于标准正态分布 <span class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(0, \mathbb{I})\)</span>。因此，公式（2）中均值前要乘以系数 <span class="math inline">\(\sqrt{1-\beta_t}\)</span>。</p>
<h2 id="reverse-process-逆向过程">Reverse Process 逆向过程</h2>
<p>前向（扩散）过程是给数据添加噪音，逆向过程就是去噪音过程。逆向过程中，我们以高斯噪声 <span class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(0, \mathbb{I})\)</span> 作为输入，如果能够得到逆向过程的分布 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>， 那么我们能够得到真实的样本。根据文献 <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-319-16859-3_42">On the theory of stochastic processes, with particular reference to applications</a>，如果 <span class="math inline">\(q(\mathbf{x}_t|\mathbf{x}_{t-1})\)</span> 满足高斯分布且 <span class="math inline">\(\beta_t\)</span> 足够小，<span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>仍然是高斯分布。 由于无法直接推断 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>，因此使用深度学习模型 <span class="math inline">\(p_\theta\)</span> 去拟合 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>，模型参数为 <span class="math inline">\(\theta\)</span>。</p>
<p><span class="math display">\[p_\theta(\mathbf{x}_{0:T}) = p_\theta(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_T) = p(\mathbf{x}_T) \prod ^T_{t=1} p_\theta (\mathbf{x}_{t-1}|\mathbf{x}_t) \tag{8}\]</span></p>
<p><span class="math display">\[p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t)) \tag{9}\]</span></p>
<h3 id="求解条件概率-qmathbfx_t-1mathbfx_t-mathbfx_0">求解条件概率 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)\)</span></h3>
<p>虽然无法直接求到 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>（注意这里是 <span class="math inline">\(q\)</span> 而不是模型的 <span class="math inline">\(p_\theta\)</span>)，在知道初始分布 <span class="math inline">\(\mathbf{x}_0\)</span> 的情况下，可以通过贝叶斯公式得到 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)\)</span> 为:</p>
<p><span class="math display">\[q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \textcolor{blue}{\tilde{\mu_t}} (\mathbf{x}_t, \mathbf{x}_0), \textcolor{red} {\tilde{\beta_t}} \mathbb{I}) \tag{10}\]</span></p>
<blockquote>
<p><strong>三变量贝叶斯公式</strong> <span class="math display">\[P(x|y,z) = \frac{P(y|x,z)P(x|z)}{P(y|z)} = \frac{P(z|x,y)P(x|y)}{P(z|y)}\]</span></p>
</blockquote>
<p>根据上文公式（2）和（7）可知，</p>
<p><span class="math display">\[q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbb{I}) = \exp \left( -\frac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{2\beta_t} \right)\]</span></p>
<p><span class="math display">\[q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\overline{\alpha}_t} \mathbf{x}_0, (1-\overline{\alpha}_t) \mathbb{I}) = \exp \left( -\frac{(\mathbf{x}_t - \sqrt{\overline{\alpha}_t} \mathbf{x}_0)^2}{2\beta_t} \right)\]</span></p>
<p>因此，公式（10）完整推导过程如下： <span class="math display">\[\begin{split}
&amp;q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0)\frac{q(\mathbf{x}_{t-1}|\mathbf{x}_0)}{q(\mathbf{x}_t|\mathbf{x}_0)}\\
&amp; \propto \exp\left( -\frac{1}{2}\left( \frac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{\beta_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\overline{\alpha}_{t-1}}\mathbf{x}_0)^2}{1 - \overline{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\overline{\alpha}_t}\mathbf{x}_0)^2}{1 - \overline{\alpha}_t} \right)\right) \\
&amp;= \exp \left( -\frac{1}{2} \left( \frac{\mathbf{x}_t^2 - 2\sqrt{\alpha_t} \mathbf{x}_t \textcolor{blue}{\mathbf{x}_{t-1}} + \alpha_t \textcolor{red}{\mathbf{x}_{t-1}^2}}{\beta_t} + \frac{\textcolor{red}{\mathbf{x}_{t-1}^2} - 2\sqrt{\overline{\alpha}_{t-1}} \mathbf{x}_0 \textcolor{blue}{\mathbf{x}_{t-1}}  + \overline{\alpha}_{t-1}\mathbf{x}_0^2}{1 - \overline{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\overline{\alpha}_t} \mathbf{x}_0)^2}{1 - \overline{\alpha}_t} \right)\right) \\
&amp;= \exp \left( - \frac{1}{2}\left(\textcolor{red}{ (\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \overline{\alpha}_{t-1}})}\mathbf{x}_{t-1}^2 - \textcolor{blue}{(\frac{2\sqrt{\alpha_t}}{\beta_t}\mathbf{x}_t + \frac{2\sqrt{\overline{\alpha}_t - 1}}{1 - \overline{\alpha}_{t-1}} \mathbf{x}_0)} \mathbf{x}_{t-1} + C(\mathbf{x}_t, \mathbf{x}_0)    \right)\right)
\end{split} \tag{11}\]</span></p>
<blockquote>
<p><strong>高斯概率密度函数</strong> <span class="math display">\[\mathcal{N}(\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma}} \exp \left(-\frac{1}{2} \left( \frac{x-\mu}{\sigma}\right)^2 \right) = \frac{1}{\sqrt{2 \pi \sigma}} \exp \left(-\frac{1}{2} \left(\frac{1}{\sigma^2}x^2 - \frac{2\mu}{\sigma^2} x + \frac{\mu^2}{\sigma^2}\right)\right)\]</span></p>
</blockquote>
<p>在上面的推导过程中，我们可以看到通过贝叶斯公式将逆向过程转换为前向过程，且最终得到的概率密度函数和高斯概率密度函数的指数部分能一一对应。结合前文 <span class="math inline">\(\alpha_t + \beta_t = 1\)</span>，我们可以建立如下对应关系：</p>
<p><span class="math display">\[\begin{split}
&amp; \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \overline{\alpha}_{t-1}} = \frac{1}{\tilde{\beta}_t} \\
&amp; \Rightarrow \tilde{\beta}_t = \frac{1}{\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \overline{\alpha}_{t-1}}} = \frac{1}{\frac{\alpha_t - \overline{\alpha}_t + \beta_t}{\beta_t(1-\overline{\alpha}_{t-1})}} \\
&amp; = \frac{\beta_t(1-\overline{\alpha}_{t-1})}{\alpha_t - \overline{\alpha}_t + \beta_t} = \frac{1 - \overline{\alpha}_{t-1}}{1 - \overline{\alpha}_t} \beta_t
\end{split}\tag{12}\]</span></p>
<p><span class="math display">\[\begin{split}
&amp;\frac{2\sqrt{\alpha_t}}{\beta_t}\mathbf{x}_t + \frac{2\sqrt{\overline{\alpha}_t - 1}}{1 - \overline{\alpha}_{t-1}} \mathbf{x}_0 = \frac{2 \tilde{\mu}_t(\mathbf{x}_t, \mathbf{x}_0)}{\tilde{\beta}_t} \\
&amp; \Rightarrow \tilde{\mu}_t(\mathbf{x}_t, \mathbf{x}_0) = \left( \frac{\sqrt{\alpha_t}}{\beta_t}\mathbf{x}_t + \frac{\sqrt{\overline{\alpha}_t - 1}}{1 - \overline{\alpha}_{t-1}} \mathbf{x}_0 \right) \tilde{\beta}_t \\
&amp;= \left( \frac{\sqrt{\alpha_t}}{\beta_t}\mathbf{x}_t + \frac{\sqrt{\overline{\alpha}_t - 1}}{1 - \overline{\alpha}_{t-1}} \mathbf{x}_0 \right) \frac{1 - \overline{\alpha}_{t-1}}{1 - \overline{\alpha}_t} \beta_t \\
&amp;= \frac{(1-\overline{\alpha}_{t-1})\sqrt{\alpha_t} \mathbf{x}_t + \beta_t \sqrt{\overline{\alpha}_{t-1}}\mathbf{x}_0}{\beta_t (1-\overline{\alpha}_{t-1})} \cdot \frac{(1 - \overline{\alpha}_{t-1})\beta_t }{1 - \overline{\alpha}_t} \\
&amp;= \frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t} \mathbf{x}_0
\end{split}\tag{13}\]</span></p>
<h3 id="求解均值-mu_thetamathbfx_t-mathbfx_0-和方差-sigma_thetamathbfx_t-mathbfx_0">求解均值 <span class="math inline">\(\mu_\theta(\mathbf{x}_t, \mathbf{x}_0)\)</span> 和方差 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, \mathbf{x}_0)\)</span></h3>
<p>通过公式（10）和（11），我们可以得到 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)\)</span> 的分布。而且，根据公式（3）<span class="math inline">\(\mathbf{x}_t = \sqrt{\overline{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\overline{\alpha_t}}\tilde{\epsilon}_t\)</span>，可以得到： <span class="math display">\[\mathbf{x}_0 = \frac{1}{\sqrt{\overline{\alpha}_t}}(\mathbf{x}_t - \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon}_t) \tag{14}\]</span></p>
<p>将公式（14）代入公式（13），可以得到如下结果： <span class="math display">\[\begin{split}
\tilde{\mu}_t(\mathbf{x}_t, \mathbf{x}_0) &amp;= \frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t} \mathbf{x}_0 \\
&amp;= \frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t} \frac{1}{\sqrt{\overline{\alpha}_t}}(\mathbf{x}_t - \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon}_t) \\
&amp;= \frac{\alpha_t(1-\overline{\alpha}_{t-1})\mathbf{x}_t}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)} + \frac{\beta_t}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)}(\mathbf{x}_t - \sqrt{1-\overline{\alpha}_t}\epsilon_t)\\
&amp;= \frac{\alpha_t \mathbf{x}_t - \overline{\alpha}_t\mathbf{x}_t + (1-\alpha_t)\mathbf{x}_t - (1-\alpha_t)\sqrt{1-\overline{\alpha}_t}\epsilon_t}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)} \\
&amp;= \frac{(1-\overline{\alpha}_t)\mathbf{x}_t - (1-\alpha_t)\sqrt{1-\overline{\alpha}_t}\epsilon_t}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)}\\
&amp;= \frac{\mathbf{x}_t}{\sqrt{\alpha_t}} - \frac{(1-\alpha_t)\epsilon_t}{\sqrt{\alpha_t}\sqrt{1-\overline{\alpha}_t}}\\
&amp;= \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_t \right)
\end{split}\tag{15}\]</span></p>
<p>之前说到，我们使用深度学习模型 <span class="math inline">\(p_\theta\)</span> 去拟合逆向过程的分布 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>， 根据公式（9）可知，<span class="math inline">\(p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t))\)</span>，我们希望训练模型 <span class="math inline">\(\mu_\theta(\mathbf{x}_t, t)\)</span> 以预估 <span class="math inline">\(\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_t \right)\)</span>。由于 <span class="math inline">\(\mathbf{x}_t\)</span> 在训练阶段的逆向过程中是输入的图片数据，因此是已知的，我们可以转而让模型去预估噪声 <span class="math inline">\(\epsilon_t\)</span>， 即令：</p>
<p><span class="math display">\[\mu_\theta (\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_\theta(\mathbf{x}_t, t) \right) \tag{16}\]</span></p>
<p>因此，</p>
<p><span class="math display">\[ \mathbf{x}_{t-1} = \mathcal{N}(\mathbf{x}_{t-1}; \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_\theta(\mathbf{x}_t, t) \right), \Sigma_\theta(\mathbf{x}_t, t) ) \tag{17}\]</span></p>
<p>方差 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, t)\)</span> 可以有多种选择，DDPM使用的是 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, t) = \tilde{\beta}_t\)</span> 且认为 <span class="math inline">\(\tilde{\beta}_t = \beta_t\)</span> 和 <span class="math inline">\(\tilde{\beta}_t = \frac{1 - \overline{\alpha}_{t-1}}{1 - \overline{\alpha}_t} \cdot \beta_t\)</span> 的结果近似，而在<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10741">GLIDE</a>中，则是根据网络预测的结果计算方差。</p>
<h3 id="ddpm-推断过程">DDPM 推断过程</h3>
<p>因此，DDPM每一步推断可以总结如下：</p>
<ul>
<li>每个时间步骤通过 <span class="math inline">\(\mathbf{x}_t\)</span> 和步骤 <span class="math inline">\(t\)</span> 来预测高斯噪声 <span class="math inline">\(\epsilon_\theta(\mathbf{x}_t, t)\)</span>，然后根据公式（16）得到均值 <span class="math inline">\(\mu_\theta(\mathbf{x}_t, t)\)</span></li>
<li>得到方差 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, t)\)</span>， DDPM使用的是 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, t) = \tilde{\beta}_t\)</span> 且认为 <span class="math inline">\(\tilde{\beta}_t = \beta_t\)</span> 和 <span class="math inline">\(\tilde{\beta}_t = \frac{1 - \overline{\alpha}_{t-1}}{1 - \overline{\alpha}_t} \cdot \beta_t\)</span> 的结果近似，</li>
<li>根据公式（9）得到 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>，利用重参数得到 <span class="math inline">\(\mathbf{x}_{t-1}\)</span></li>
</ul>
<figure>
<img src="/2022/10/29/DDPM/DDPM_Forw_Rev.png" srcset="/img/loading.gif" lazyload alt="图2 DDDPM前向与逆向数据转换过程，可以看到逆向过程中 \mathbf{x}_0 和 \mathbf{x}_t 之间反复横跳"><figcaption aria-hidden="true">图2 DDDPM前向与逆向数据转换过程，可以看到逆向过程中 <span class="math inline">\(\mathbf{x}_0\)</span> 和 <span class="math inline">\(\mathbf{x}_t\)</span> 之间反复横跳</figcaption>
</figure>
<h2 id="diffusion-模型训练">Diffusion 模型训练</h2>
<p>逆向阶段是让模型去预估噪声 <span class="math inline">\(\epsilon_\theta(\mathbf{x}_t, t)\)</span>，从而产生较好的 <span class="math inline">\(\mu_\theta(\mathbf{x}_t, t)\)</span> 和 <span class="math inline">\(\Sigma_\theta(\mathbf{x}_t, t)\)</span>， 以求得概率 <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>。 作者的目标是在真实数据分布下，最大化模型预测分布的对数似然函数，即优化 <span class="math inline">\(\mathcal{x}_0 \sim q(\mathcal{x}_0)\)</span> 下的 <span class="math inline">\(p_\theta(\mathcal{x}_0)\)</span> 的交叉熵。</p>
<p><span class="math display">\[\mathcal{L} =\mathbb{E}_{q(\mathbf{x}_0)} \left[ -\log p_\theta(\mathcal{x}_0) \right] \tag{18} \]</span></p>
<p>和变分自动编码器类似，使用变分下限（Variantional Lower Bound, VLB）也称ELBO（Evidence Lower Bound），来优化 <span class="math inline">\(-\log p_\theta(\mathbf{x}_0)\)</span>：</p>
<p><span class="math display">\[\begin{split}
-\log p_\theta(\mathbf{x}_0) &amp;\le -\log p_\theta(\mathbf{x}_0) + KL(q(\mathbf{x}_{1:T}|\mathbf{x}_0)||p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)) \\
&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \left[\log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] \\
&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \left[\log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})/p_\theta(\mathbf{x}_0)}\right] \\
&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \left[\log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} + \underbrace{\log p_\theta(\mathbf{x}_0)}_{与q无关}\right] \\
&amp;= \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \left[\log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \right] \\
\end{split}\tag{19}\]</span></p>
<blockquote>
<p><strong>KL散度</strong> <span class="math display">\[KL(P||Q) = \sum_i P(x_i) \log \frac{P(x_i)}{Q(x_i)}\]</span></p>
<p><strong>Fubini定理</strong></p>
<p>如果 <span class="math inline">\(\int_{A\times B} |f(x,y)| d(x,y) &lt; \infty\)</span>, 则下式成立 <span class="math display">\[\int_A \left(\int_B f(x,y) dy\right)dx = \int_B \left(\int_A f(x,y) dx\right)dy = \int_{A\times B} f(x,y) d(x,y)\]</span></p>
</blockquote>
<h3 id="fubini定理推断mathcall_vlb">Fubini定理推断<span class="math inline">\(\mathcal{L}_{VLB}\)</span></h3>
<p>根据公式（18），对公式（19）左右两边取期望 <span class="math inline">\(\mathbb{E}_{q(\mathbf{x}_0)}\)</span>，利用重积分中的Fubini定理： <span class="math display">\[\begin{split}
\mathcal{L}_{VLB} &amp;= \mathbb{E}_{q(\mathbf{x}_0)}\left( \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \left[\log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \right]  \right) = \mathbb{E}_{q(\mathbf{x_{0:T}})} \left[ \log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})}  \right] \\
&amp; \ge \mathbb{E}_{q(\mathbf{x}_0)} \left[ -\log p_\theta(\mathcal{x}_0) \right]
\end{split}\tag{20}\]</span></p>
<p>最小化 <span class="math inline">\(\mathcal{L}_{VLB}\)</span> 即可最小化我们的目标损失函数，即公式（18）。</p>
<h3 id="jensen不等式推断mathcall_vlb">Jensen不等式推断<span class="math inline">\(\mathcal{L}_{VLB}\)</span></h3>
<blockquote>
<p><strong>Jensen不等式概率论版本</strong></p>
<p>对于随机变量<span class="math inline">\(X\)</span>，<span class="math inline">\(\varphi\)</span> 是任意凸函数，则下式成立 <span class="math display">\[\varphi(E(X)) \le E(\varphi(X))\]</span></p>
</blockquote>
<p><span class="math display">\[\begin{split}
\mathcal{L} &amp;= \mathbb{E}_{q(\mathbf{x}_0)} \left[ -\log p_\theta(\mathcal{x}_0) \right] \\
&amp;= - \mathbb{E}_{q(\mathbf{x}_0)} \log \left( p_\theta(\mathbf{x}_0) \cdot \int p_\theta(\mathbf{x}_{1:T}) d\mathbf{x}_{1:T} \right) \\
&amp;= - \mathbb{E}_{q(\mathbf{x}_0)}\log\left( \int p_\theta(\mathbf{x}_{0:T})d\mathbf{x}_{1:T} \right)\\
&amp;= - \mathbb{E}_{q(\mathbf{x}_0)}\log\left( \int q(\mathbf{x}_{1:T}|\mathbf{x}_0) \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} d\mathbf{x}_{1:T} \right)\\
&amp;= - \mathbb{E}_{q(\mathbf{x}_0)}\log\left( \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \right)\\
&amp;= \mathbb{E}_{q(\mathbf{x}_0)}\log\left( \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \right)\\
&amp;\le \mathbb{E}_{q(\mathbf{x}_{0:T})} \log \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} = \mathcal{L}_{VLB}
\end{split} \tag{21}\]</span></p>
<p>可以看到，通过Fubini定理和Jensen不等式都可以得到相同的结果。这里需要解释下公式（21）中几个步骤。</p>
<ul>
<li>第2行到第3行：因为 <span class="math inline">\(p_\theta(\mathbf{x}_0)\)</span> 和 <span class="math inline">\(\mathbf{x}_{1:T}\)</span> 无关，因此可以将 <span class="math inline">\(p_\theta(\mathbf{x}_0)\)</span> 看作常数项与 <span class="math inline">\(p_\theta(\mathbf{x}_{1:T})\)</span> 相乘。根据联合概率密度函数定义，我们可以看出 <span class="math inline">\(p_\theta(\mathbf{x}_{0:T}) = p_\theta(\mathbf{x}_0) p_\theta(\mathbf{x}_{1:T})\)</span></li>
<li>第6行到第7行：将 <span class="math inline">\(\log\)</span> 函数看作 <span class="math inline">\(\varphi\)</span>，根据Jensen不等式可知不等号成立。且根据 <span class="math inline">\(p_\theta(\mathbf{x}_{0:T}) = p_\theta(\mathbf{x}_0) p_\theta(\mathbf{x}_{1:T})\)</span>，我们可以看出，对 <span class="math inline">\(q(\mathbf{x}_0)\)</span> 和 <span class="math inline">\(q(\mathbf{x}_{1:T}|\mathbf{x}_0)\)</span> 连续积分可以得到 <span class="math inline">\(q(\mathbf{x}_{0:T})\)</span></li>
</ul>
<h3 id="进一步拆解-_vlb">进一步拆解 $_{VLB} $</h3>
<p>进一步对 <span class="math inline">\(\mathcal{L}_{VLB}\)</span> 推导，可以得到熵与多个KL散度的累加，具体推导如下：</p>
<p><span class="math display">\[\begin{split}
\mathcal{L}_{VLB} &amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \log \frac{q(\mathcal{x}_{1:T}|\mathcal{x}_0)}{p_\theta(\mathcal{x}_{0:T})} \right]\\
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \log \frac{\prod^T_{t=1} q(\mathbf{x}_t|\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_T)\prod^T_{t=1}p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} \right]\\
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=1} \log \frac{q(\mathbf{x}_t|\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} \right]\\
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} + \log \frac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)} \right]\\
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=2} \log \left( \frac{q(\mathbf{x}_t|\mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} \cdot \frac{q(\mathbf{x}_t|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)} \right) + \log \frac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)} \right]\\
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)} \right]\\
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} + \log \frac{q(\mathbf{x}_T|\mathbf{x}_0)}{q(\mathbf{x}_{1}|\mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)} \right]\\
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ - \log p_\theta(\mathbf{x}_T) + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}  + \log q(\mathbf{x}_1|\mathbf{x}_0) - \log p_\theta(\mathbf{x}_0|\mathbf{x}_1) \right] \\
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \log \frac{q(\mathbf{x}_T|\mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} + \sum^T_{t=2} \log \frac{q(\mathbf{x}_t|\mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} - \log p_\theta(\mathbf{x}_0|\mathbf{x}_1) \right]\\
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \underbrace{KL(q(\mathbf{x}_T|\mathbf{x}_0)||p_\theta(\mathbf{x}_T))}_{L_T} + \sum^T_{t=2} \underbrace{KL(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)||p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))}_{L_{t-1}} - \underbrace{\log p_\theta(\mathbf{x}_0|\mathbf{x}_1)}_{L_0} \right]
\end{split}\tag{22}\]</span></p>
<p>公式（22）的解释： + 第1行到第2行：分子是根据公式（1）得来。分母是根据公式（8）得来 + 第4行到第5行：我们可以看到，第四行中 <span class="math inline">\(\log\)</span> 符号后分式中分母保持不变，只是分子 <span class="math inline">\(q(\mathbf{x}_t|\mathbf{x}_{t-1})\)</span> 通过贝叶斯公式转换： <span class="math display">\[q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \frac{q(\mathbf{x}_t, \mathbf{x}_{t-1}|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)} = \frac{q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_0) \cdot p(\mathbf{x}_{t}|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)} \]</span></p>
<p>公式（22）可以重新写为以下公式： <span class="math display">\[\mathcal{L}_{VLB} = L_T + L_{T-1} + \cdots + L_0 \tag{23-1}\]</span> <span class="math display">\[L_T = KL(q(\mathbf{x}_T|\mathbf{x}_0)||p_\theta(\mathbf{x}_T)) \tag{23-2}\]</span> <span class="math display">\[L_t = KL(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)||p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)); \quad 1 \le t \le T-1 \tag{23-3}\]</span> <span class="math display">\[L_0 = - \log p_\theta(\mathbf{x}_0|\mathbf{x}_1) \tag{23-4}\]</span></p>
<h3 id="ddpm的loss">DDPM的LOSS</h3>
<p>由于前向过程 <span class="math inline">\(q\)</span> 没有可学习的参数，而且 <span class="math inline">\(\mathcal{x}_T\)</span> 是纯高斯噪声，因此公式（23-2）中的 <span class="math inline">\(L_T\)</span> 可以当作常量忽略。最后一项 <span class="math inline">\(L_0\)</span> 作者使用离散分段累计函数来计算，可以避免计算协方差等过程，但该项对最终结果影响不大。 <span class="math inline">\(L_t\)</span> 可以看作是拉近两个变量的高斯分布，即公式（10）<span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \textcolor{blue}{\tilde{\mu_t}} (\mathbf{x}_t, \mathbf{x}_0), \textcolor{red} {\tilde{\beta_t}} \mathbb{I})\)</span> 和公式（9）<span class="math inline">\(p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t))\)</span>，根据<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions">多元高斯分布的KL散度求解</a>，可以得到下式： <span class="math display">\[L_t = \mathbb{E}_{q(\mathbf{x}_{0:T})}\left[\frac{1}{2||\Sigma_\theta(\mathbf{x}_t, \mathbf{x}_0)||_2^2}||\tilde{\mu}(\mathbf{x}_t, \mathbf{x}_0) - \mu_\theta(\mathbf{x}_t, t)||^2 \right]+C_t \tag{24}\]</span></p>
<blockquote>
<p><strong>多元高斯分布KL散度解释</strong></p>
<p><span class="math display">\[KL(p||q) = \frac{(\mu_1 - \mu_2)^2}{2 \sigma_2^2} + \frac{\sigma_1^2}{2\sigma_2^2} + \log \frac{\sigma_2}{\sigma_p} - \frac{1}{2} \]</span> 由于DDPM中只有均值是与参数 <span class="math inline">\(\theta\)</span> 相关，因此，这里后三项都是与参数 <span class="math inline">\(\theta\)</span> 无关的常数。</p>
</blockquote>
<p>其中公式（24）中参数 <span class="math inline">\(C\)</span> 是与模型参数 <span class="math inline">\(\theta\)</span> 无关的常量。将公式（15）和公式（16）的结果代入公式（24）可以得到： <span class="math display">\[\begin{split}
L_t &amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon}\left[\frac{1}{2||\Sigma_\theta(\mathbf{x}_t, \mathbf{x}_0)||_2^2}||\tilde{\mu}(\mathbf{x}_t, \mathbf{x}_0) - \mu_\theta(\mathbf{x}_t, t)||^2 \right]\\ 
&amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon}\left[\frac{1}{2||\Sigma_\theta||_2^2}\left|\left|\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_t \right) - \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} \epsilon_\theta(\mathbf{x}_t, t) \right)\right|\right|^2 \right]\\
&amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ \frac{1}{2||\Sigma_\theta||_2^2} \left|\left| \frac{1}{\sqrt{\overline{\alpha}_t}} \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}} (\epsilon_t - \epsilon_\theta(\mathbf{x}_t, t)) \right|\right|^2\right] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ \frac{(1-\alpha_t)^2}{2 \alpha_t (1-\overline{\alpha}_t) ||\Sigma_\theta||^2_2} ||\epsilon_t - \epsilon_\theta(\mathbf{x}_t, t)||^2 \right] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ \frac{(1-\alpha_t)^2}{2 \alpha_t (1-\overline{\alpha}_t) ||\Sigma_\theta||^2_2} ||\epsilon_t - \epsilon_\theta(\sqrt{\overline{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon_t}, t)||^2 \right] \\
\end{split}\tag{25}\]</span></p>
<p>公式（25）中最后一行是根据公式（3）将 <span class="math inline">\(\mathbf{x}_t\)</span> 替换为 <span class="math inline">\(\mathbf{x}_0\)</span>。因此DDPM的Loss狠心是学习高斯噪声 <span class="math inline">\(\epsilon_t\)</span> 和 <span class="math inline">\(\epsilon_\theta\)</span> 之间的MSE。</p>
<h2 id="ddpm最终算法">DDPM最终算法</h2>
<p>DDPM最终算法流程如下，其中Training表示训练阶段，Sampling表示逆向阶段。根据代码理解，Train完成之后，通过Sampling生成图像。 <img src="/2022/10/29/DDPM/DDPM_Total_Alg.png" srcset="/img/loading.gif" lazyload alt="DDPM最终算法"></p>
<p><strong>训练阶段的步骤：</strong></p>
<ul>
<li><p>从数据集中采样 <span class="math inline">\(\mathbf{x}_0\)</span></p></li>
<li><p>随机选取时间步骤<span class="math inline">\(t\)</span></p></li>
<li><p>生成高斯噪声 <span class="math inline">\(\epsilon_t \in \mathcal{N}(0, \mathbb{I})\)</span></p></li>
<li><p>预估 <span class="math inline">\(\epsilon_\theta(\sqrt{\overline{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon_t}, t)\)</span></p></li>
<li><p>计算MSE Loss: <span class="math inline">\(||\epsilon_t - \epsilon_\theta(\sqrt{\overline{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_t} \tilde{\epsilon_t}, t)||^2\)</span>，并利用反向传播算法训练模型</p></li>
</ul>
<p><strong>逆向阶段采用如下步骤进行采样：</strong></p>
<ul>
<li><p>从高斯分布中采样 <span class="math inline">\(\mathcal{x}_T\)</span></p></li>
<li><p>按照 <span class="math inline">\(T,T-1,\cdots,1\)</span>的顺序进行迭代</p>
<ul>
<li><p>如果 <span class="math inline">\(t\)</span> = 1，令 <span class="math inline">\(z=0\)</span>；如果 <span class="math inline">\(t&gt;1\)</span>，则从高斯分布中采样 <span class="math inline">\(z\sim\mathcal{N}(0, \mathbb{I})\)</span></p></li>
<li><p>利用公式（16）学习出均值 <span class="math inline">\(\mu_\theta(\mathbf{x}_t, t)\)</span>，并利用公式（12）计算方差 <span class="math inline">\(\sigma_t\)</span></p></li>
<li><p>通过重参数技巧采样 <span class="math inline">\(\mathbf{x}_{t-1} = \mu_\theta(\mathbf{x}_t, t) + \sigma_t z\)</span></p></li>
</ul></li>
<li><p>通过以上步骤恢复出 <span class="math inline">\(\mathbf{x}_0\)</span></p></li>
</ul>
<h1 id="代码解读">代码解读</h1>
<p>Coming soon</p>
<h1 id="ddim加速diffusion采样和方差的选择">DDIM：加速Diffusion采样和方差的选择</h1>
<p>DDPM的高质量生成依赖较大的 <span class="math inline">\(T\)</span>, 这就导致Diffusion的前向过程非常缓慢，因此有作者提出一种牺牲多样性来换取更快推断的手段，提出了 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.02502">Denoising diffusion implicit model (DDIM)</a>。</p>
<p>根据公式（3）及高斯分布可加性，可以得到 <span class="math inline">\(\mathbf{x}_{t-1}\)</span> 为： <span class="math display">\[\begin{split}
\mathbf{x}_{t-1} &amp;= \sqrt{\overline{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1 - \overline{\alpha}_{t-1}} \tilde{\epsilon}_{t-1} \\
&amp;= \sqrt{\overline{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_{t-1}-\sigma^2_t}\tilde{\epsilon}_t + \sigma_t \epsilon_t \\
&amp;= \sqrt{\overline{\alpha}_{t-1}} 
\mathbf{x}_0 + \sqrt{1 - \overline{\alpha}_{t-1} - \sigma^2_t} \left( \frac{\mathbf{x}_t - \sqrt{\overline{\alpha}_t}\mathbf{x}_0}{\sqrt{1-\overline{\alpha}_t}} \right) + \sigma_t \epsilon_t
\end{split}\tag{26}\]</span></p>
<p>根据上文，我们可以得知 <span class="math inline">\(\epsilon_t \sim \mathcal{N}(0,1)\)</span> 是服从标准正态分布的噪声, <span class="math inline">\(\tilde{\epsilon}_t \sim \mathcal{N}(0,1)\)</span> 是根据正态分布可加性变换过的且服从标准正态分布的噪声。 公式（26）第二行是根据高斯分布可加性推导而来， 假定 $P_A =  <em>{t-1} (0, (1-</em>{t-1}) ) <span class="math inline">\(，\)</span>P_B = (0, _t^2) $，我们可以得到如下推断： <span class="math display">\[\begin{split}
P_A &amp;= P_A - P_B + P_B\\
&amp;= \sqrt{\overline{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1-\overline{\alpha}_{t-1}-\sigma^2_t}\tilde{\epsilon}_t + \sigma_t \epsilon_t \\
\end{split}\tag{27}\]</span> 公式（26）第二行到第三行也是根据公式（3）变换而来： <span class="math display">\[\begin{split}
&amp; \mathbf{x}_{t} = \sqrt{\overline{\alpha}_{t}} \mathbf{x}_0 + \sqrt{1 - \overline{\alpha}_{t}} \tilde{\epsilon}_{t} \\ 
&amp; \Rightarrow \tilde{\epsilon}_t = \frac{\mathbf{x}_t - \sqrt{\overline{\alpha}_t}\mathbf{x}_0}{\sqrt{1-\overline{\alpha}_t}}
\end{split}\tag{28}\]</span> 因此可以重写公式（10）和（11）为 <span class="math display">\[q_\sigma (\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\overline{\alpha}_{t-1}} 
\mathbf{x}_0 + \sqrt{1 - \overline{\alpha}_{t-1} - \sigma^2_t} \left( \frac{\mathbf{x}_t - \sqrt{\overline{\alpha}_t}\mathbf{x}_0}{\sqrt{1-\overline{\alpha}_t}} \right), \sigma_t^2 \mathbb{I}) \tag{29}\]</span></p>
<p>不同于公式（10）和（16），公式（26）将方差 <span class="math inline">\(\sigma_t^2\)</span> 引入到了均值当中，当 <span class="math inline">\(\sigma_t^2 = \tilde{\beta}_t = \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t} \beta_t\)</span> 时，公式（26）等价于公式（10）。 在DDIM中将由公式（26）得到的 <span class="math inline">\(q_\sigma (\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)\)</span> 称为非马尔可夫过程，因为 <span class="math inline">\(\mathbf{x}_t\)</span> 的分布同时依赖 <span class="math inline">\(\mathbf{x}_{t-1}\)</span> 和 <span class="math inline">\(\mathbf{x}_0\)</span>。DDIM进一步定义了 <span class="math inline">\(\sigma_t(\eta)^2 = \eta \cdot \tilde{\beta}_t\)</span>。当 <span class="math inline">\(\eta=0\)</span> 时，diffusion的采样过程会丧失所有随机性从而得到一个确定性的结果，但是可以改变 <span class="math inline">\(\mathbf{x}_T\)</span>。而当 <span class="math inline">\(\eta = 1\)</span> 时，DDIM等驾驭DDPM （使用 <span class="math inline">\(\tilde{\beta}_t\)</span>作为方差的版本），用随机性换取生成性能。</p>
<p>对于方差 <span class="math inline">\(\sigma_t^2\)</span> 的选择，可以总结如下：</p>
<ul>
<li>DDPM:
<ul>
<li><span class="math inline">\(\sigma^2_{t,\theta} = \Sigma_\theta(\mathbf{x}_t, t)\)</span> 相当于模型学习的方差，DDPM称为learned，实际没有使用，GLIDE使用了这种方差。</li>
<li><span class="math inline">\(\sigma^2_{t,s} = \tilde{\beta}_t = \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t} \beta_t\)</span>，DDPM称为fixedsamll，用于CelebA-HQ数据集和LSUN。</li>
<li><span class="math inline">\(\sigma^2_{t,l} = \beta_t\)</span>，DDPM称之为fixedlarge，用于CIFAR10数据集，注意 <span class="math inline">\(\sigma_{t,l}&gt;\sigma_{t,s}\)</span></li>
</ul></li>
<li>DDIM:
<ul>
<li><span class="math inline">\(\sigma_t(\eta)^2 = \eta \cdot \tilde{\beta}_t\)</span>，DDIM是在fixedsmall版本上再乘以一个系数 <span class="math inline">\(\eta\)</span>。</li>
</ul></li>
</ul>
<p>假设总的采样步骤是 <span class="math inline">\(T\)</span>, 采样间隔是 <span class="math inline">\(Q\)</span>，DDIM的采样步数为 <span class="math inline">\(S=T/Q\)</span>，<span class="math inline">\(S\)</span> 和 <span class="math inline">\(\eta\)</span> 的实验结果如下： <img src="/2022/10/29/DDPM/DDIM_Exp.png" srcset="/img/loading.gif" lazyload alt="DDIM的实验结果"> 可以看到在 <span class="math inline">\(S\)</span> 很小的时候 <span class="math inline">\(\eta = 0\)</span> 取得了最好的结果，而当步骤 <span class="math inline">\(T\)</span> 足够大的时候，使用更大的方差 <span class="math inline">\(\sigma_t^2\)</span> 可以得到更好的结果。</p>
<h1 id="参考">参考</h1>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models?</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/525106459">由浅入深了解Diffusion Model - 知乎</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/576475987">扩散模型（Diffusion Model）简要介绍于源码分析</a></li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" class="category-chain-item">论文阅读</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">#神经网络</a>
      
        <a href="/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/">#生成模型</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Denoising Diffusion Probabilistic Model (DDPM) 论文阅读</div>
      <div>http://watsonlee.github.io/2022/10/29/DDPM/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Huacheng Li</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>October 29, 2022</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/11/18/KKT/" title="约束优化">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">约束优化</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/10/29/hello-world/" title="Hello World">
                        <span class="hidden-mobile">Hello World</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
